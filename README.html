<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Image classification using fine-tuned ViT - for historical document sorting</title>
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.8.1/github-markdown-dark.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
		<style>
			body {
				margin: 0;
			}

			.markdown-body-content {
				box-sizing: border-box;
				min-width: 200px;
				max-width: 980px;
				margin: 0 auto;
				padding: 45px;
			}

			@media (max-width: 767px) {
				.markdown-body-content {
					padding: 15px;
				}
			}

			.markdown-body {
				--base-size-8: 8px;
				--base-size-16: 16px;
			}
		</style>
	</head>
	<body class="markdown-body">
		<article class="markdown-body-content"><h1>Image classification using fine-tuned ViT - for historical document sorting</h1>
<h3>Goal: solve a task of archive page images sorting (for their further content-based processing)</h3>
<p><strong>Scope:</strong> Processing of images, training / evaluation of ViT model,<br>
input file/directory processing, class ğŸª§  (category) results of top<br>
N predictions output, predictions summarizing into a tabular format,<br>
HF ğŸ˜Š hub <sup><a href="#user-content-fn-1-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-1-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”— support for the model, multiplatform (Win/Lin) data<br>
preparation scripts for PDF to PNG conversion</p>
<h3>Table of contents ğŸ“‘</h3>
<ul>
<li><a href="#versions-">Versions ğŸ</a></li>
<li><a href="#model-description-">Model description ğŸ“‡</a>
<ul>
<li><a href="#data-">Data ğŸ“œ</a></li>
<li><a href="#categories-">Categories ğŸª§ï¸</a></li>
</ul>
</li>
<li><a href="#how-to-install-">How to install ğŸ”§</a></li>
<li><a href="#how-to-run-prediction--modes">How to run prediction ğŸª„ modes</a>
<ul>
<li><a href="#page-processing-">Page processing ğŸ“„</a></li>
<li><a href="#directory-processing-">Directory processing ğŸ“</a></li>
</ul>
</li>
<li><a href="#results-">Results ğŸ“Š</a>
<ul>
<li><a href="#result-tables-and-their-columns-">Result tables and their columns ğŸ“ğŸ“‹</a></li>
</ul>
</li>
<li><a href="#data-preparation-">Data preparation ğŸ“¦</a>
<ul>
<li><a href="#pdf-to-png-">PDF to PNG ğŸ“š</a></li>
<li><a href="#png-pages-annotation-">PNG pages annotation ğŸ”</a></li>
<li><a href="#png-pages-sorting-for-training-">PNG pages sorting for training ğŸ“¬</a></li>
</ul>
</li>
<li><a href="#for-developers-">For developers ğŸª›</a>
<ul>
<li><a href="#training-">Training ğŸ’ª</a></li>
<li><a href="#evaluation-">Evaluation ğŸ†</a></li>
</ul>
</li>
<li><a href="#contacts-">Contacts ğŸ“§</a></li>
<li><a href="#acknowledgements-">Acknowledgements ğŸ™</a></li>
<li><a href="#appendix-">Appendix ğŸ¤“</a></li>
</ul>
<hr>
<h2>Versions ğŸ</h2>
<p>There are currently 2 version of the model available for download, both of them have the same set of categories,<br>
but different data annotations. The latest approved <code class="notranslate">v2.1</code> is considered to be default and can be found in the <code class="notranslate">main</code> branch<br>
of HF ğŸ˜Š hub <sup><a href="#user-content-fn-1-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-1-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”—</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th align="right">Version</th>
<th>Base</th>
<th align="center">Pages</th>
<th align="center">PDFs</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code class="notranslate">v2.0</code></td>
<td><code class="notranslate">vit-base-patch16-224</code></td>
<td align="center">10073</td>
<td align="center"><strong>3896</strong></td>
<td align="left">annotations with mistakes, more heterogenous data</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v2.1</code></td>
<td><code class="notranslate">vit-base-patch16-224</code></td>
<td align="center">11940</td>
<td align="center"><strong>5002</strong></td>
<td align="left"><code class="notranslate">main</code>: more diverse pages in each category, less annotation mistakes</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v2.2</code></td>
<td><code class="notranslate">vit-base-patch16-224</code></td>
<td align="center">15855</td>
<td align="center"><strong>5730</strong></td>
<td align="left">same data as <code class="notranslate">v2.1</code> + some restored pages from <code class="notranslate">v2.0</code></td>
</tr>
<tr>
<td align="right"><code class="notranslate">v3.2</code></td>
<td><code class="notranslate">vit-base-patch16-384</code></td>
<td align="center">15855</td>
<td align="center"><strong>5730</strong></td>
<td align="left">same data as <code class="notranslate">v2.2</code>, but a bit larger model base with higher resolution</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v5.2</code></td>
<td><code class="notranslate">vit-large-patch16-384</code></td>
<td align="center">15855</td>
<td align="center"><strong>5730</strong></td>
<td align="left">same data as <code class="notranslate">v2.2</code>, but the largest model base with higher resolution</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<details>
<summary>Base model - size ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th><strong>Version</strong></th>
<th><strong>Disk space</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="notranslate">vit-base-patch16-224</code></td>
<td>344 Mb</td>
</tr>
<tr>
<td><code class="notranslate">vit-base-patch16-384</code></td>
<td>345 Mb</td>
</tr>
<tr>
<td><code class="notranslate">vit-large-patch16-384</code></td>
<td>1.2 Gb</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<h2>Model description ğŸ“‡</h2>
<p><a target="_blank" rel="noopener noreferrer" href="architecture.png"><img src="architecture.png" alt="architecture.png" style="max-width: 100%;"></a></p>
<p>ğŸ”² <strong>Fine-tuned</strong> model repository: UFAL's <strong>vit-historical-page</strong> <sup><a href="#user-content-fn-1-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-1-3-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”—</p>
<p>ğŸ”³ <strong>Base</strong> model repository: Google's <strong>vit-base-patch16-224</strong>,  <strong>vit-base-patch16-384</strong>,  <strong>vit-large-patch16-284</strong> <sup><a href="#user-content-fn-2-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> <sup><a href="#user-content-fn-13-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-13-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> <sup><a href="#user-content-fn-14-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-14-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> ğŸ”—</p>
<p>The model was trained on the manually âœï¸ annotated dataset of historical documents, in particular, images of pages<br>
from the archival documents with paper sources that were scanned into digital form.</p>
<p>The images contain various combinations of texts ï¸ğŸ“„, tables ğŸ“, drawings ğŸ“ˆ, and photos ğŸŒ„ -<br>
categories ğŸª§ described <a href="#categories-">below</a> were formed based on those archival documents. Page examples can be found in<br>
the <a href="category_samples">category_samples</a> ğŸ“ directory.</p>
<p>The key <strong>use case</strong> of the provided model and data processing pipeline is to classify an input PNG image from PDF scanned<br>
paper source into one of the categories - each responsible for the following content-specific data processing pipeline.</p>
<blockquote>
<p>In other words, when several APIs for different OCR subtasks are at your disposal - run this classifier first to<br>
mark the input data as machine-typed (old style fonts) / handwritten âœï¸ / just printed plain ï¸ğŸ“„ text<br>
or structured in tabular ğŸ“ format text, as well as to mark the presence of the printed ğŸŒ„ or drawn ğŸ“ˆ graphic<br>
materials yet to be extracted from the page images.</p>
</blockquote>
<h3>Data ğŸ“œ</h3>
<p><strong>Training</strong> ğŸ’ª set of the model: <strong>8950</strong> images for <code class="notranslate">v2.0</code></p>
<p><strong>Training</strong> ğŸ’ª set of the model: <strong>10745</strong> images for <code class="notranslate">v2.1</code></p>
<p><strong>Training</strong> ğŸ’ª set of the model: <strong>14565</strong> images for <code class="notranslate">v2.2</code>, <code class="notranslate">v3.2</code> and <code class="notranslate">v5.2</code></p>
<blockquote>
<p><strong>90% of all</strong> - proportion in categories ğŸª§ tabulated <a href="#categories-">below</a></p>
</blockquote>
<p><strong>Evaluation</strong> ğŸ† set:  <strong>1290</strong> images (taken from <code class="notranslate">v2.2</code> annotations)</p>
<blockquote>
<p><strong>10% of all</strong> - same proportion in categories ğŸª§ as <a href="#categories-">below</a> and demonstrated in <a href="result%2Ftables%2F20250526-1158_model_v22_TOP-1_EVAL.csv">model_EVAL.csv</a> ğŸ“</p>
</blockquote>
<p>Manual âœï¸ annotation was performed beforehand and took some time âŒ›, the categories ğŸª§  were formed from<br>
different sources of the archival documents originated in the 1920-2020 years span.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>Disproportion of the categories ğŸª§ in both training data and provided evaluation <a href="category_samples">category_samples</a> ğŸ“ is<br>
<strong>NOT</strong> intentional, but rather a result of the source data nature.</p>
</div>
<p>In total, several thousands of separate PDF files were selected and split into PNG pages, ~4k of scanned documents<br>
were one-page long which covered around a third of all data, and ~2k of them were much longer (dozens and hundreds<br>
of pages) covering the rest (more than 60% of all annotated data).</p>
<p>The specific content and language of the<br>
source data is irrelevant considering the model's vision resolution, however, all of the data samples were from <strong>archaeological<br>
reports</strong> which may somehow affect the drawing detection preferences due to the common form of objects being ceramic pieces,<br>
arrowheads, and rocks formerly drawn by hand and later illustrated with digital tools (examples can be found in<br>
<a href="category_samples%2FDRAW">category_samples/DRAW</a> ğŸ“)</p>
<h3>Categories ğŸª§</h3>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th align="right">Labelï¸</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code class="notranslate">DRAW</code></td>
<td align="left"><strong>ğŸ“ˆ - drawings, maps, paintings, schematics, or graphics, potentially containing some text labels or captions</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">DRAW_L</code></td>
<td align="left"><strong>ğŸ“ˆğŸ“ - drawings, etc but presented within a table-like layout or includes a legend formatted as a table</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">LINE_HW</code></td>
<td align="left"><strong>âœï¸ğŸ“ - handwritten text organized in a tabular or form-like structure</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">LINE_P</code></td>
<td align="left"><strong>ğŸ“ - printed text organized in a tabular or form-like structure</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">LINE_T</code></td>
<td align="left"><strong>ğŸ“ - machine-typed text organized in a tabular or form-like structure</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">PHOTO</code></td>
<td align="left"><strong>ğŸŒ„ - photographs or photographic cutouts, potentially with text captions</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">PHOTO_L</code></td>
<td align="left"><strong>ğŸŒ„ğŸ“ - photos presented within a table-like layout or accompanied by tabular annotations</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">TEXT</code></td>
<td align="left"><strong>ğŸ“° - mixtures of printed, handwritten, and/or typed text, potentially with minor graphical elements</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">TEXT_HW</code></td>
<td align="left"><strong>âœï¸ğŸ“„ - only handwritten text in paragraph or block form (non-tabular)</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">TEXT_P</code></td>
<td align="left"><strong>ğŸ“„ - only printed text in paragraph or block form (non-tabular)</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">TEXT_T</code></td>
<td align="left"><strong>ğŸ“„ - only machine-typed text in paragraph or block form (non-tabular)</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p>The categories were chosen to sort the pages by the following criteria:</p>
<ul>
<li><strong>presence of graphical elements</strong> (drawings ğŸ“ˆ OR photos ğŸŒ„)</li>
<li><strong>type of text</strong> ğŸ“„ (handwritten âœï¸ï¸ OR printed OR typed OR mixed ğŸ“°)</li>
<li><strong>presence of tabular layout / forms</strong> ğŸ“</li>
</ul>
<blockquote>
<p>The reasons for such distinction are different processing pipelines for different types of pages, which would be<br>
applied after the classification as mentioned <a href="#model-description-">above</a>.</p>
</blockquote>
<p>Examples of pages sorted by category ğŸª§ can be found in the <a href="category_samples">category_samples</a> ğŸ“ directory<br>
which is also available as a testing subset of the training data (can be used to run evaluation and prediction with a<br>
necessary <code class="notranslate">--inner</code> flag).</p>
<hr>
<h2>How to install ğŸ”§</h2>
<p>Step-by-step instructions on this program installation are provided here. The easiest way to obtain the model would<br>
be to use the HF ğŸ˜Š hub repository <sup><a href="#user-content-fn-1-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-1-4-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”— that can be easily accessed via this project.</p>
<details>
<summary>Hardware requirements ğŸ‘€</summary>
<p><strong>Minimal</strong> machine ğŸ–¥ï¸ requirements for slow prediction run (and very slow training / evaluation):</p>
<ul>
<li><strong>CPU</strong> with a decent (above average) operational memory size</li>
</ul>
<p><strong>Ideal</strong> machine ğŸ–¥ï¸ requirements for fast prediction (and relatively fast training / evaluation):</p>
<ul>
<li><strong>CPU</strong> of some kind and memory size</li>
<li><strong>GPU</strong> (for real CUDA <sup><a href="#user-content-fn-10-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-10-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> support - only one of Nvidia's cards)</li>
</ul>
</details>
<div class="markdown-alert markdown-alert-warning"><p class="markdown-alert-title"><svg class="octicon octicon-alert mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Warning</p><p>Make sure you have <strong>Python version 3.10+</strong> installed on your machine ğŸ’» and check its<br>
<strong>hardware requirements</strong> for correct program running provided above.<br>
Then create a separate virtual environment for this project</p>
</div>
<details>
<summary>How to ğŸ‘€</summary>
<p>Clone this project to your local machine ğŸ–¥ï¸ï¸ via:</p>
<pre class="notranslate"><code class="notranslate">cd /local/folder/for/this/project
git init
git clone https://github.com/ufal/atrium-page-classification.git
</code></pre>
<p><strong>OR</strong> for updating the already cloned project with some changes, go to the folder containing (hidden) <code class="notranslate">.git</code><br>
subdirectory and run pulling which will merge upcoming files with your local changes:</p>
<pre class="notranslate"><code class="notranslate">cd /local/folder/for/this/project/atrium-page-classification
git add &lt;changed_file&gt;
git commit -m 'local changes'
git pull -X theirs
</code></pre>
<p>Alternatively, if you do <strong>NOT</strong> care about local changes <strong>OR</strong> you want to get the latest project files,<br>
just remove those files (all <code class="notranslate">.py</code>, <code class="notranslate">.txt</code> and <code class="notranslate">README</code> files) and pull the latest version from the repository:</p>
<pre class="notranslate"><code class="notranslate">cd /local/folder/for/this/project/atrium-page-classification
rm *.py
rm *.txt
rm README*
git pull
</code></pre>
<p>Next step would be a creation of the virtual environment. Follow the <strong>Unix</strong> / <strong>Windows</strong>-specific<br>
instruction at the venv docs <sup><a href="#user-content-fn-3-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-3-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup> ğŸ‘€ğŸ”— if you don't know how to.</p>
<p>After creating the venv folder, activate the environment via:</p>
<pre class="notranslate"><code class="notranslate">source &lt;your_venv_dir&gt;/bin/activate
</code></pre>
<p>and then inside your virtual environment, you should install Python libraries (takes time âŒ›)</p>
</details>
<div class="markdown-alert markdown-alert-caution"><p class="markdown-alert-title"><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>Up to <strong>1 GB of space for model</strong> files and checkpoints is needed, and up to <strong>7 GB<br>
of space for the Python libraries</strong> (Pytorch and its dependencies, etc)</p>
</div>
<p>Installation of Python dependencies can be done via:</p>
<pre class="notranslate"><code class="notranslate">pip install -r requirements.txt
</code></pre>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>The so-called <strong>CUDA <sup><a href="#user-content-fn-10-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-10-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> support</strong> for Python's PyTorch library is supposed to be automatically installed<br>
at this point - when the presence of the GPU on your machine ğŸ–¥ï¸<br>
is checked for the first time, later it's also checked every time before the model initialization<br>
(for training, evaluation or prediction run).</p>
</div>
<p>After the dependencies installation is finished successfully, in the same virtual environment, you can<br>
run the Python program.</p>
<p>To test that everything works okay and see the flag<br>
descriptions call for <code class="notranslate">--help</code> â“:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py -h
</code></pre>
<p>You should see a (hopefully) helpful message about all available command line flags. Your next step would be<br>
to <strong>pull the model from the HF ğŸ˜Š hub repository <sup><a href="#user-content-fn-1-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-1-5-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”—</strong> via:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --hf
</code></pre>
<p><strong>OR</strong> for specific model version (e.g. <code class="notranslate">main</code>, <code class="notranslate">v2.0</code> or <code class="notranslate">vX.2</code>) use the <code class="notranslate">--revision</code> flag:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --hf -rev v2.0
</code></pre>
<p><strong>OR</strong> for specific base model version (e.g. <code class="notranslate">google/vit-large-patch16-384</code>) use the <code class="notranslate">--base</code> flag (only when the<br>
trained model version demands such base model as described <a href="#versions-">above</a>):</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --hf -rev v5.2 -b google/vit-large-patch16-384
</code></pre>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>If you already have the model files in the <code class="notranslate">model/movel_&lt;revision&gt;</code><br>
directory next to this file, you do <strong>NOT</strong> have to use the <code class="notranslate">--hf</code> flag to download the<br>
model files from the HF ğŸ˜Š repo <sup><a href="#user-content-fn-1-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-1-6-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”— (only for the <strong>model version update</strong>).</p>
</div>
<p>You should see a message about loading the model from the hub and then saving it locally on<br>
your machine ğŸ–¥ï¸.</p>
<p>Only after you have obtained the trained model files (takes less time âŒ› than installing dependencies),<br>
you can play with any commands provided <a href="#how-to-run-prediction--modes">below</a>.</p>
<p>After the model is downloaded, you should see a similar file structure:</p>
<details>
<summary>Initial project tree ğŸŒ³ files structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">/local/folder/for/this/project/atrium-page-classification
â”œâ”€â”€ model
    â””â”€â”€ movel_&lt;revision&gt; 
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â””â”€â”€ preprocessor_config.json
â”œâ”€â”€ checkpoint
    â”œâ”€â”€ models--google--vit-base-patch16-224
        â”œâ”€â”€ blobs
        â”œâ”€â”€ snapshots
        â””â”€â”€ refs
    â””â”€â”€ .locs
        â””â”€â”€ models--google--vit-base-patch16-224
â”œâ”€â”€ data_scripts
    â”œâ”€â”€ windows
        â”œâ”€â”€ move_single.bat
        â”œâ”€â”€ pdf2png.bat
        â””â”€â”€ sort.bat
    â””â”€â”€ unix
        â”œâ”€â”€ move_single.sh
        â”œâ”€â”€ pdf2png.sh
        â””â”€â”€ sort.sh
â”œâ”€â”€ result
    â”œâ”€â”€ plots
        â”œâ”€â”€ date-time_conf_mat.png
        â””â”€â”€ ...
    â””â”€â”€ tables
        â”œâ”€â”€ date-time_TOP-N.csv
        â”œâ”€â”€ date-time_TOP-N_EVAL.csv
        â”œâ”€â”€ date-time_EVAL_RAW.csv
        â””â”€â”€ ...
â”œâ”€â”€ category_samples
    â”œâ”€â”€ DRAW
        â”œâ”€â”€ CTX193200994-24.png
        â””â”€â”€ ...
    â”œâ”€â”€ DRAW_L
    â””â”€â”€ ...
â”œâ”€â”€ run.py
â”œâ”€â”€ classifier.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.txt
â”œâ”€â”€ README.md
â””â”€â”€ ...
</code></pre>
</details>
<p>Some of the folders may be missing, like mentioned <a href="#for-developers-">later</a> <code class="notranslate">model_output</code> which is automatically created<br>
only after launching the model.</p>
<hr>
<h2>How to run prediction ğŸª„ modes</h2>
<p>There are two main ways to run the program:</p>
<ul>
<li><strong>Single PNG file classification</strong> ğŸ“„</li>
<li><strong>Directory with PNG files classification</strong> ğŸ“</li>
</ul>
<p>To begin with, open <a href="config.txt">config.txt</a> âš™ and change folder path in the <code class="notranslate">[INPUT]</code> section, then<br>
optionally change <code class="notranslate">top_N</code> and <code class="notranslate">batch</code> in the <code class="notranslate">[SETUP]</code> section.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>ï¸ <strong>Top-3</strong> is enough to cover most of the images, setting <strong>Top-5</strong> will help with a small number<br>
of difficult to classify samples.</p>
</div>
<p>The <code class="notranslate">batch</code> variable value depends on your machine ğŸ–¥ï¸ memory size</p>
<details>
<summary>Rough estimations of memory usage per batch size ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th><strong>Batch size</strong></th>
<th><strong>CPU / GPU memory usage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>2 Gb</td>
</tr>
<tr>
<td>8</td>
<td>3 Gb</td>
</tr>
<tr>
<td>16</td>
<td>5 Gb</td>
</tr>
<tr>
<td>32</td>
<td>9 Gb</td>
</tr>
<tr>
<td>64</td>
<td>17 Gb</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p>It is safe to use batch size below <strong>12</strong> for a regular office desktop computer, and lower it to <strong>4</strong> if it's an old device.<br>
For training on a High Performance Computing cluster, you may use values above <strong>20</strong> for<br>
the <code class="notranslate">batch</code> variable in the <code class="notranslate">[SETUP]</code> section.</p>
<div class="markdown-alert markdown-alert-caution"><p class="markdown-alert-title"><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>Do <strong>NOT</strong> try to change <strong>base_model</strong> and other section contents unless you know what you are doing</p>
</div>
<details>
<summary>Rough estimations of disk space needed for trained model in relation to the base model ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th><strong>Version</strong></th>
<th><strong>Disk space</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="notranslate">vit-base-patch16-224</code></td>
<td>344 Mb</td>
</tr>
<tr>
<td><code class="notranslate">vit-base-patch16-384</code></td>
<td>345 Mb</td>
</tr>
<tr>
<td><code class="notranslate">vit-large-patch16-384</code></td>
<td>1.2 Gb</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p>Make sure the virtual environment with all the installed libraries is activated, you are in the project<br>
directory with Python files and only then proceed.</p>
<details>
<summary>How to ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">cd /local/folder/for/this/project/
source &lt;your_venv_dir&gt;/bin/activate
cd atrium-page-classification
</code></pre>
</details>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>All the listed below commands for Python scripts running are adapted for <strong>Unix</strong> consoles, while<br>
<strong>Windows</strong> users must use <code class="notranslate">python</code> instead of <code class="notranslate">python3</code> syntax</p>
</div>
<h3>Page processing ğŸ“„</h3>
<p>The following prediction should be run using the <code class="notranslate">-f</code> or <code class="notranslate">--file</code> flag with the path argument. Optionally,<br>
you can use the <code class="notranslate">-tn</code> or <code class="notranslate">--topn</code> flag with the number of guesses you want to get, and also the <code class="notranslate">-m</code> or<br>
<code class="notranslate">--model</code> flag with the path to the model folder argument.</p>
<details>
<summary>How to ğŸ‘€</summary>
<p>Run the program from its starting point <a href="run.py">run.py</a> ğŸ“ with optional flags:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py -tn 3 -f '/full/path/to/file.png' -m '/full/path/to/model/folder'
</code></pre>
<p>for exactly TOP-3 guesses with a console output.</p>
<p><strong>OR</strong> if you are sure about default variables set in the <a href="config.txt">config.txt</a> âš™:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py -f '/full/path/to/file.png'
</code></pre>
<p>to run a single PNG file classification - the output will be in the console.</p>
</details>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>Console output and all result tables contain <strong>normalized</strong> scores for the highest N class ğŸª§  scores</p>
</div>
<h3>Directory processing ğŸ“</h3>
<p>The following prediction type does <strong>NOT</strong> require explicit directory path setting with the <code class="notranslate">-d</code> or <code class="notranslate">--directory</code>,<br>
since its default value is set in the <a href="config.txt">config.txt</a> âš™ file and awakens when the <code class="notranslate">--dir</code> flag<br>
is used. The same flags for the number of guesses and the model folder path as for the single-page<br>
processing can be used. In addition, 2 directory-specific flags  <code class="notranslate">--inner</code> and <code class="notranslate">--raw</code> are available.</p>
<div class="markdown-alert markdown-alert-caution"><p class="markdown-alert-title"><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>You must either explicitly set the <code class="notranslate">-d</code> flag's argument or use the <code class="notranslate">--dir</code> flag (calling for the preset in<br>
<code class="notranslate">[INPUT]</code> section default value of the input directory) to process PNG files on the directory<br>
level, otherwise, nothing will happen</p>
</div>
<p>Worth mentioning that the <strong>directory ğŸ“ level processing is performed in batches</strong>, therefore you should refer to<br>
the hardware's memory capacity requirements for different batch sizes tabulated <a href="#how-to-run-prediction--modes">above</a>.</p>
<details>
<summary>How to ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">python3 run.py -tn 3 -d '/full/path/to/directory' -m '/full/path/to/model/folder'
</code></pre>
<p>for exactly TOP-3 guesses in tabular format from all images found in the given directory.</p>
<p><strong>OR</strong> if you are really sure about default variables set in the <a href="config.txt">config.txt</a> âš™:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --dir 

python3 run.py -rev v3.2 -b google/vit-base-patch16-384 --inner --dir
</code></pre>
</details>
<p>The classification results of PNG pages collected from the directory will be saved ğŸ’¾ to related <a href="result">results</a> ğŸ“<br>
folders defined in <code class="notranslate">[OUTPUT]</code> section of <a href="config.txt">config.txt</a> âš™ file.</p>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>To additionally get raw class ğŸª§ probabilities from the model along with the TOP-N results, use<br>
<code class="notranslate">--raw</code> flag when processing the directory (<strong>NOT</strong> available for single file processing)</p>
</div>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>To process all PNG files in the directory <strong>AND its subdirectories</strong> use the <code class="notranslate">--inner</code> flag<br>
when processing the directory, or switch its default value to <code class="notranslate">True</code> in the <code class="notranslate">[SETUP]</code> section</p>
</div>
<p>Naturally, processing of the large amount of PNG pages takes time âŒ› and progress of this process<br>
is recorded in the console via messages like <code class="notranslate">Processed &lt;BÃ—N&gt; images</code> where <code class="notranslate">B</code><br>
is batch size set in the <code class="notranslate">[SETUP]</code> section of the <a href="config.txt">config.txt</a> âš™ file,<br>
and <code class="notranslate">N</code> is an iteration of the current dataloader processing loop.</p>
<p>Only after all images from the input directory are processed, the output table is<br>
saved ğŸ’¾ in the <code class="notranslate">result/tables</code> folder.</p>
<hr>
<h2>Results ğŸ“Š</h2>
<p>There are accuracy performance measurements and plots of confusion matrices for the evaluation<br>
dataset (10% of the provided in <code class="notranslate">[TRAIN]</code>'s folder data). Both graphic plots and tables with<br>
results can be found in the <a href="result">result</a> ğŸ“ folder.</p>
<p><code class="notranslate">v2.0</code> Evaluation set's accuracy (<strong>Top-3</strong>):  <strong>95.58%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-3 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20250526-1147_model_v20_conf_mat_TOP-3.png"><img src="result%2Fplots%2F20250526-1147_model_v20_conf_mat_TOP-3.png" alt="TOP-3 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v2.1</code> Evaluation set's accuracy (<strong>Top-3</strong>):  <strong>99.84%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-3 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20250526-1157_model_v21_conf_mat_TOP-3.png"><img src="result%2Fplots%2F20250526-1157_model_v21_conf_mat_TOP-3.png" alt="TOP-3 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v2.2</code> Evaluation set's accuracy (<strong>Top-3</strong>):  <strong>100.00%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-3 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20250526-1201_model_v22_conf_mat_TOP-3.png"><img src="result%2Fplots%2F20250526-1201_model_v22_conf_mat_TOP-3.png" alt="TOP-3 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v2.0</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>84.96%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20250526-1152_model_v20_conf_mat_TOP-1.png"><img src="result%2Fplots%2F20250526-1152_model_v20_conf_mat_TOP-1.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v2.1</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>96.36%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20250526-1156_model_v21_conf_mat_TOP-1.png"><img src="result%2Fplots%2F20250526-1156_model_v21_conf_mat_TOP-1.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v2.2</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>99.61%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20250526-1202_model_v22_conf_mat_TOP-1.png"><img src="result%2Fplots%2F20250526-1202_model_v22_conf_mat_TOP-1.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<blockquote>
<p><strong>Confusion matrices</strong> provided above show the diagonal of matching gold and predicted categories ğŸª§<br>
while their off-diagonal elements show inter-class errors. By those graphs you can judge<br>
<strong>what type of mistakes to expect</strong> from your model.</p>
</blockquote>
<p>By running tests on the evaluation dataset after training you can generate the following output files:</p>
<ul>
<li><strong>date-time_model_TOP-N_EVAL.csv</strong> - (by default) results of the evaluation dataset with TOP-N guesses</li>
<li><strong>date-time_model_conf_mat_TOP-N.png</strong> - (by default) confusion matrix plot for the evaluation dataset also with TOP-N guesses</li>
<li><strong>date-time_model_EVAL_RAW.csv</strong> - (by flag <code class="notranslate">--raw</code>) raw probabilities for all classes of the evaluation dataset</li>
</ul>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>Generated tables will be sorted by <strong>FILE</strong> and <strong>PAGE</strong> number columns in ascending order.</p>
</div>
<p>Additionally, results of prediction inference run on the directory level without checked results are included.</p>
<h3>Result tables and their columns ğŸ“ğŸ“‹</h3>
<details>
<summary>General result tables ğŸ‘€</summary>
<p>Demo files  <code class="notranslate">v2.0</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> (small): <a href="result%2Ftables%2Fmodel_1119_3_TOP-5.csv">model_TOP-5.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20250526-1142_model_v20_TOP-3_EVAL.csv">model_TOP-3_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20250526-1148_model_v20_TOP-1_EVAL.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values: <a href="result%2Ftables%2F20250314-1600_model_1119_3_TOP-5.csv">model_TOP-5.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20250314-1615_model_1119_3_TOP-3.csv">model_TOP-3.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v2.1</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20250526-1153_model_v21_TOP-3_EVAL.csv">model_TOP-3_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20250526-1151_model_v21_TOP-1_EVAL.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values: <a href="result%2Ftables%2F20250417-1138_model_672_3_TOP-3.csv">model_TOP-3.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20250417-1244_model_672_3_TOP-3.csv">model_TOP-3.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v2.2</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20250526-1156_model_v22_TOP-3_EVAL.csv">model_TOP-3_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20250526-1158_model_v22_TOP-1_EVAL.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
</ul>
<p>With the following <strong>columns</strong> ğŸ“‹:</p>
<ul>
<li><strong>FILE</strong> - name of the file</li>
<li><strong>PAGE</strong> - number of the page</li>
<li><strong>CLASS-N</strong> - label of the category ğŸª§, guess TOP-N</li>
<li><strong>SCORE-N</strong> - score of the category ğŸª§, guess TOP-N</li>
</ul>
<p>and optionally</p>
<ul>
<li><strong>TRUE</strong> - actual label of the category ğŸª§</li>
</ul>
</details>
<details>
<summary>Raw result tables ğŸ‘€</summary>
<p>Demo files <code class="notranslate">v2.0</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset <strong>RAW</strong>: <a href="result%2Ftables%2F20250526-1148_model_v20_EVAL_RAW.csv">model_RAW_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values <strong>RAW</strong>: <a href="result%2Ftables%2F20250314-1600_model_1119_3_RAW.csv">model_RAW.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small) <strong>RAW</strong>: <a href="result%2Ftables%2F20250314-1615_model_1119_3_RAW.csv">model_RAW.csv</a> ğŸ“</p>
</li>
</ul>
<p>Demo files <code class="notranslate">v2.1</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset <strong>RAW</strong>: <a href="result%2Ftables%2F20250526-1151_model_v21_EVAL_RAW.csv">model_RAW_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values <strong>RAW</strong>: <a href="result%2Ftables%2F20250417-1242_model_672_3_RAW.csv">model_RAW.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small) <strong>RAW</strong>: <a href="result%2Ftables%2F20250417-1244_model_672_3_RAW.csv">model_RAW.csv</a> ğŸ“</p>
</li>
<li>
<p>Demo files <code class="notranslate">v2.2</code>:</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset <strong>RAW</strong>: <a href="result%2Ftables%2F20250526-1156_model_v22_EVAL_RAW.csv">model_RAW_EVAL.csv</a> ğŸ“</p>
</li>
</ul>
<p>With the following <strong>columns</strong> ğŸ“‹:</p>
<ul>
<li><strong>FILE</strong> - name of the file</li>
<li><strong>PAGE</strong> - number of the page</li>
<li><strong>&lt;CATEGORY_LABEL&gt;</strong> - separate columns for each of the defined classes ğŸª§</li>
<li><strong>TRUE</strong> - actual label of the category ğŸª§</li>
</ul>
</details>
<p>The reason to use the <code class="notranslate">--raw</code> flag is the possible convenience of results review,<br>
since the rows will be basically sorted by categories, and most ambiguous ones will<br>
have more small probabilities instead of zeros than the most obvious (for the model)<br>
categories ğŸª§.</p>
<hr>
<h2>Data preparation ğŸ“¦</h2>
<p>You can use this section as a guide for creating your own dataset of pages, which will be suitable for<br>
further model processing.</p>
<p>There are useful multiplatform scripts in the <a href="data_scripts">data_scripts</a> ğŸ“ folder for the whole process of data preparation.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>The <code class="notranslate">.sh</code> scripts are adapted for <strong>Unix</strong> OS and <code class="notranslate">.bat</code> scripts are adapted for <strong>Windows</strong> OS, yet<br>
their functionality remains the same</p>
</div>
<p>On <strong>Windows</strong> you must also install the following software before converting PDF documents to PNG images:</p>
<ul>
<li>ImageMagick <sup><a href="#user-content-fn-5-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-5-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup> ğŸ”— - download and install the latest version</li>
<li>Ghostscript <sup><a href="#user-content-fn-6-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-6-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup> ğŸ”— - download and install the latest version (32 or 64-bit) by AGPL</li>
</ul>
<h3>PDF to PNG ğŸ“š</h3>
<p>The source set of PDF documents must be converted to page-specific PNG images before processing. The following steps<br>
describe the procedure of converting PDF documents to PNG images suitable for training, evaluation, or prediction inference.</p>
<p>Firstly, copy the PDF-to-PNG converter script to the directory with PDF documents.</p>
<details>
<summary>How to ğŸ‘€</summary>
<p><strong>Windows</strong>:</p>
<pre class="notranslate"><code class="notranslate">move \local\folder\for\this\project\data_scripts\pdf2png.bat \full\path\to\your\folder\with\pdf\files
</code></pre>
<p><strong>Unix</strong>:</p>
<pre class="notranslate"><code class="notranslate">cp /local/folder/for/this/project/data_scripts/pdf2png.sh /full/path/to/your/folder/with/pdf/files
</code></pre>
</details>
<p>Now check the content and comments in <a href="data_scripts%2Funix%2Fpdf2png.sh">pdf2png.sh</a> ğŸ“ or <a href="data_scripts%2Fwindows%2Fpdf2png.bat">pdf2png.bat</a> ğŸ“<br>
script, and run it.</p>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>You can optionally comment out the <strong>removal of processed PDF files</strong> from the script, yet it's <strong>NOT</strong><br>
recommended in case you are going to launch the program several times from the same location.</p>
</div>
<details>
<summary>How to ğŸ‘€</summary>
<p><strong>Windows</strong>:</p>
<pre class="notranslate"><code class="notranslate">cd \full\path\to\your\folder\with\pdf\files
pdf2png.bat
</code></pre>
<p><strong>Unix</strong>:</p>
<pre class="notranslate"><code class="notranslate">cd /full/path/to/your/folder/with/pdf/files
pdf2png.sh
</code></pre>
</details>
<p>After the program is done, you will have a directory full of document-specific subdirectories<br>
containing page-specific images with a similar structure:</p>
<details>
<summary>Unix folder tree ğŸŒ³ structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">/full/path/to/your/folder/with/pdf/files
â”œâ”€â”€ PdfFile1Name
    â”œâ”€â”€ PdfFile1Name-001.png
    â”œâ”€â”€ PdfFile1Name-002.png
    â””â”€â”€ ...
â”œâ”€â”€ PdfFile2Name
    â”œâ”€â”€ PdfFile2Name-01.png
    â”œâ”€â”€ PDFFile2Name-02.png
    â””â”€â”€ ...
â”œâ”€â”€ PdfFile3Name
    â””â”€â”€ PdfFile3Name-1.png 
â”œâ”€â”€ PdfFile4Name
â””â”€â”€ ...
</code></pre>
</details>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>The page numbers are padded with zeros (on the left) to match the length of the last page number in each PDF file,<br>
this is done automatically by the pdftoppm command used on <strong>Unix</strong>. While ImageMagick's <sup><a href="#user-content-fn-5-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-5-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup> ğŸ”— convert command used<br>
on <strong>Windows</strong> does <strong>NOT</strong> pad the page numbers.</p>
</div>
<details>
<summary>Windows folder tree ğŸŒ³ structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">\full\path\to\your\folder\with\pdf\files
â”œâ”€â”€ PdfFile1Name
    â”œâ”€â”€ PdfFile1Name-1.png
    â”œâ”€â”€ PdfFile1Name-2.png
    â””â”€â”€ ...
â”œâ”€â”€ PdfFile2Name
    â”œâ”€â”€ PdfFile2Name-1.png
    â”œâ”€â”€ PDFFile2Name-2.png
    â””â”€â”€ ...
â”œâ”€â”€ PdfFile3Name
    â””â”€â”€ PdfFile3Name-1.png 
â”œâ”€â”€ PdfFile4Name
â””â”€â”€ ...
</code></pre>
</details>
<p>Optionally you can use the <a href="data_scripts%2Funix%2Fmove_single.sh">move_single.sh</a> ğŸ“ or <a href="data_scripts%2Fwindows%2Fmove_single.bat">move_single.bat</a> ğŸ“ script to move<br>
all PNG files from directories with a single PNG file inside to the common directory of one-pagers.</p>
<p>By default, the scripts assume that the <code class="notranslate">onepagers</code> is the back-off directory for PDF document names without a<br>
corresponding separate directory of PNG pages found in the PDF files directory (already converted to<br>
subdirectories of pages).</p>
<details>
<summary>How to ğŸ‘€</summary>
<p><strong>Windows</strong>:</p>
<pre class="notranslate"><code class="notranslate">move \local\folder\for\this\project\atrium-page-classification\data_scripts\move_single.bat \full\path\to\your\folder\with\pdf\files
cd \full\path\to\your\folder\with\pdf\files
move_single.bat
</code></pre>
<p><strong>Unix</strong>:</p>
<pre class="notranslate"><code class="notranslate">cp /local/folder/for/this//project/atrium-page-classification/data_scripts/move_single.sh /full/path/to/your/folder/with/pdf/files
cd /full/path/to/your/folder/with/pdf/files 
move_single.sh 
</code></pre>
</details>
<p>The reason for such movement is simply convenience in the following annotation process <a href="#png-pages-annotation-">below</a>.<br>
These changes are cared for in the next <a href="data_scripts%2Funix%2Fsort.sh">sort.sh</a> ğŸ“ and <a href="data_scripts%2Fwindows%2Fsort.bat">sort.bat</a> ğŸ“ scripts as well.</p>
<h3>PNG pages annotation ğŸ”</h3>
<p>The generated PNG images of document pages are used to form the annotated gold data.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>It takes a lot of time âŒ› to collect at least several hundred examples per category.</p>
</div>
<p>Prepare a CSV table with exactly 3 columns:</p>
<ul>
<li><strong>FILE</strong> - name of the PDF document which was the source of this page</li>
<li><strong>PAGE</strong> - number of the page (<strong>NOT</strong> padded with 0s)</li>
<li><strong>CLASS</strong> - label of the category ğŸª§</li>
</ul>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>Prepare equal-in-size categories ğŸª§ if possible, so that the model will not be biased towards the over-represented labels ğŸª§</p>
</div>
<p>For <strong>Windows</strong> users, it's <strong>NOT</strong> recommended to use MS Excel for writing CSV tables, the free<br>
alternative may be Apache's OpenOffice <sup><a href="#user-content-fn-9-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-9-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup> ğŸ”—. As for <strong>Unix</strong> users, the default LibreCalc should be enough to<br>
correctly write a comma-separated CSV table.</p>
<details>
<summary>Table in .csv format example ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">FILE,PAGE,CLASS
PdfFile1Name,1,Label1
PdfFile2Name,9,Label1
PdfFile1Name,11,Label3
...
</code></pre>
</details>
<h3>PNG pages sorting for training ğŸ“¬</h3>
<p>Cluster the annotated data into separate folders using the <a href="data_scripts%2Funix%2Fsort.sh">sort.sh</a> ğŸ“ or <a href="data_scripts%2Fwindows%2Fsort.bat">sort.bat</a> ğŸ“<br>
script to copy data from the source folder to the training folder where each category ğŸª§ has its own subdirectory.<br>
This division of PNG images will be used as gold data in training and evaluation.</p>
<div class="markdown-alert markdown-alert-warning"><p class="markdown-alert-title"><svg class="octicon octicon-alert mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Warning</p><p>It does <strong>NOT</strong> matter from which directory you launch the sorting script, but you must check the top of the script for<br>
(<strong>1</strong>) the path to the previously described <strong>CSV table with annotations</strong>, (<strong>2</strong>) the path to the previously described<br>
directory containing <strong>document-specific subdirectories of page-specific PNG pages</strong>, and (<strong>3</strong>) the path to the directory<br>
where you want to store the <strong>training data of label-specific directories with annotated page images</strong>.</p>
</div>
<details>
<summary>How to ğŸ‘€</summary>
<p><strong>Windows</strong>:</p>
<pre class="notranslate"><code class="notranslate">sort.bat
</code></pre>
<p><strong>Unix</strong>:</p>
<pre class="notranslate"><code class="notranslate">sort.sh
</code></pre>
</details>
<p>After the program is done, you will have a directory full of label-specific subdirectories<br>
containing document-specific pages with a similar structure:</p>
<details>
<summary>Unix folder tree ğŸŒ³ structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">/full/path/to/your/folder/with/train/pages
â”œâ”€â”€ Label1
    â”œâ”€â”€ PdfFileAName-00N.png
    â”œâ”€â”€ PdfFileBName-0M.png
    â””â”€â”€ ...
â”œâ”€â”€ Label2
â”œâ”€â”€ Label3
â”œâ”€â”€ Label4
â””â”€â”€ ...
</code></pre>
</details>
<details>
<summary>Windows folder tree ğŸŒ³ structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">\full\path\to\your\folder\with\train\pages
â”œâ”€â”€ Label1
    â”œâ”€â”€ PdfFileAName-N.png
    â”œâ”€â”€ PdfFileBName-M.png
    â””â”€â”€ ...
â”œâ”€â”€ Label2
â”œâ”€â”€ Label3
â”œâ”€â”€ Label4
â””â”€â”€ ...
</code></pre>
</details>
<p>The sorting script can help you in moderating mislabeled samples before the training. Accurate data annotation<br>
directly affects the model performance.</p>
<p>Before running the training, make sure to check the <a href="config.txt">config.txt</a> âš™ï¸ file for the <code class="notranslate">[TRAIN]</code> section variables, where you should<br>
set a path to the data folder. Make sure label directory names do <strong>NOT</strong> contain special characters like spaces, tabs or paragraph splits.</p>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>In the <a href="config.txt">config.txt</a> âš™ï¸ file tweak the parameter of <code class="notranslate">max_categ</code><br>
for a maximum number of samples per category ğŸª§, in case you have <strong>over-represented labels</strong> significantly dominating in size.<br>
Set <code class="notranslate">max_categ</code> higher than the number of samples in the largest category ğŸª§ to use <strong>all</strong> data samples.</p>
</div>
<p>From this point, you can start model training or evaluation process.</p>
<hr>
<h2>For developers ğŸª›</h2>
<p>You can use this project code as a base for your own image classification tasks. The detailed guide on<br>
the key phases of the whole process (settings, training, evaluation) is provided here.</p>
<details>
<summary>Project files description ğŸ“‹ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>File Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="notranslate">classifier.py</code></td>
<td>Model-specific classes and related functions including predefined values for training arguments</td>
</tr>
<tr>
<td><code class="notranslate">utils.py</code></td>
<td>Task-related algorithms</td>
</tr>
<tr>
<td><code class="notranslate">run.py</code></td>
<td>Starting point of the program with its main function - can be edited for flags and function argument extensions</td>
</tr>
<tr>
<td><code class="notranslate">config.txt</code></td>
<td>Changeable variables for the program - should be edited</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p>Most of the changeable variables are in the <a href="config.txt">config.txt</a> âš™ file, specifically,<br>
in the <code class="notranslate">[TRAIN]</code>, <code class="notranslate">[HF]</code>, and <code class="notranslate">[SETUP]</code> sections.</p>
<p>In the dev sections of the configuration âš™ file, you will find many boolean variables that can be changed from the default <code class="notranslate">False</code><br>
state to <code class="notranslate">True</code>, yet it's recommended to awaken those variables solely through the specific<br>
<strong>command line flags implemented for each of these boolean variables</strong>.</p>
<p>For more detailed training process adjustments refer to the related functions in <a href="classifier.py">classifier.py</a> ğŸ“<br>
file, where you will find some predefined values not used in the <a href="run.py">run.py</a> ğŸ“ file.</p>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>For both training and evaluation, you must make sure that the training pages directory is set right in the<br>
<a href="config.txt">config.txt</a> âš™ and it contains category ğŸª§ subdirectories with images inside.<br>
Names of the category ğŸª§ subdirectories are sorted in the alphabetic order and become actual<br>
label names and replace the default categories ğŸª§ list</p>
</div>
<p>Device ğŸ–¥ï¸ requirements for training / evaluation:</p>
<ul>
<li><strong>CPU</strong> of some kind and memory size</li>
<li><strong>GPU</strong> (for real CUDA <sup><a href="#user-content-fn-10-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-10-3-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> support - better one of Nvidia's cards)</li>
</ul>
<p>Worth mentioning that the efficient training is possible only with a CUDA-compatible GPU card.</p>
<details>
<summary>Rough estimations of memory usage ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th><strong>Batch size</strong></th>
<th><strong>CPU / GPU memory usage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>2 Gb</td>
</tr>
<tr>
<td>8</td>
<td>3 Gb</td>
</tr>
<tr>
<td>16</td>
<td>5 Gb</td>
</tr>
<tr>
<td>32</td>
<td>9 Gb</td>
</tr>
<tr>
<td>64</td>
<td>17 Gb</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p>For test launches on the <strong>CPU-only device ğŸ–¥ï¸</strong> you should set <strong>batch size to lower than 4</strong>, and even in this<br>
case, <strong>above-average CPU memory capacity</strong> is a must-have to avoid a total system crush.</p>
<h3>Training ğŸ’ª</h3>
<p>To train the model run:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --train
</code></pre>
<p>The training process has an automatic progress logging into console, and should take approximately 5-12h<br>
depending on your machine's ğŸ–¥ï¸ CPU / GPU memory size and prepared dataset size.</p>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>Run the training with <strong>default hyperparameters</strong> if you have at least ~10,000 and <strong>less than 50,000 page samples</strong><br>
of the very similar to the initial source data - meaning, no further changes are required for fine-tuning model<br>
for the same task on an expanded (or new) dataset of document pages, even number of categories ğŸª§ does<br>
<strong>NOT</strong> matter while it stays under <strong>20</strong></p>
</div>
<details>
<summary>Training hyperparameters ğŸ‘€</summary>
<ul>
<li>eval_strategy "epoch"</li>
<li>save_strategy "epoch"</li>
<li>learning_rate <strong>5e-5</strong></li>
<li>per_device_train_batch_size 8</li>
<li>per_device_eval_batch_size 8</li>
<li>num_train_epochs <strong>3</strong></li>
<li>warmup_ratio <strong>0.1</strong></li>
<li>logging_steps <strong>10</strong></li>
<li>load_best_model_at_end True</li>
<li>metric_for_best_model "accuracy"</li>
</ul>
</details>
<p>Above are the default hyperparameters or TrainingArguments <sup><a href="#user-content-fn-11-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-11-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup> used in the training process that can be partially<br>
(only <code class="notranslate">epoch</code> and <code class="notranslate">log_step</code>) changed in the <code class="notranslate">[TRAIN]</code> section, plus <code class="notranslate">batch</code> in the <code class="notranslate">[SETUP]</code>section,<br>
of the <a href="config.txt">config.txt</a> âš™ file.</p>
<blockquote>
<p>You are free to play with the <strong>learning rate</strong> right in the training function arguments called in the <a href="run.py">run.py</a> ğŸ“ file,<br>
yet <strong>warmup ratio and other hyperparameters</strong> are accessible only through the <a href="classifier.py">classifier.py</a> ğŸ“ file.</p>
</blockquote>
<p>Playing with training hyperparameters is<br>
recommended only if <strong>training ğŸ’ª loss</strong> (error rate) descends too slow to reach 0.001-0.001<br>
values by the end of the 3rd (last by default) epoch.</p>
<p>In the case <strong>evaluation ğŸ† loss</strong> starts to steadily going up after the previous descend, this means<br>
you have reached the limit of worthy epochs, and next time you should set <code class="notranslate">epochs</code> to the<br>
number of epoch that has successfully ended before you noticed the evaluation loss growth.</p>
<p>During training image transformations <sup><a href="#user-content-fn-12-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-12-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup> are applied sequentially with a 50% chance.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>No rotation, reshaping, or flipping was applied to the images, mainly color manipulations were used. The<br>
reason behind this are pages containing specific form types, general text orientation on the pages, and the default<br>
reshape of the model input to the square 224x224 resolution images.</p>
</div>
<details>
<summary>Image preprocessing steps ğŸ‘€</summary>
<ul>
<li>transforms.ColorJitter(<strong>brightness</strong> 0.5)</li>
<li>transforms.ColorJitter(<strong>contrast</strong> 0.5)</li>
<li>transforms.ColorJitter(<strong>saturation</strong> 0.5)</li>
<li>transforms.ColorJitter(<strong>hue</strong> 0.5)</li>
<li>transforms.Lambda(lambda img: ImageEnhance.<strong>Sharpness</strong>(img).enhance(random.uniform(0.5, 1.5)))</li>
<li>transforms.Lambda(lambda img: img.filter(ImageFilter.<strong>GaussianBlur</strong>(radius=random.uniform(0, 2))))</li>
</ul>
</details>
<p>More about selecting the image transformation and the available ones you can read in the PyTorch torchvision docs <sup><a href="#user-content-fn-12-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-12-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup>.</p>
<p>After training is complete the model will be saved ğŸ’¾ to its separate subdirectory in the <code class="notranslate">model</code> directory, by default,<br>
the <strong>naming of the model folder</strong> corresponds to the length of its training batch dataloader and the number of epochs -<br>
for example <code class="notranslate">model_&lt;S/B&gt;_E</code> where <code class="notranslate">E</code> is the number of epochs, <code class="notranslate">B</code> is the batch size, and <code class="notranslate">S</code> is the size of your<br>
<strong>training</strong> dataset (by defaults, 90% of the provided in <code class="notranslate">[TRAIN]</code>'s folder data).</p>
<details>
<summary>Full project tree ğŸŒ³ files structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">/local/folder/for/this/project/atrium-page-classification
â”œâ”€â”€ model
    â”œâ”€â”€ movel_v&lt;HFrevision1&gt; 
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â””â”€â”€ preprocessor_config.json
    â”œâ”€â”€ movel_v&lt;HFrevision2&gt;
    â””â”€â”€ ...
â”œâ”€â”€ checkpoint
    â”œâ”€â”€ models--google--vit-base-patch16-224
        â”œâ”€â”€ blobs
        â”œâ”€â”€ snapshots
        â””â”€â”€ refs
    â””â”€â”€ .locs
        â””â”€â”€ models--google--vit-base-patch16-224
â”œâ”€â”€ model_output
    â”œâ”€â”€ checkpoint-version1
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â”œâ”€â”€ trainer_state.json
        â”œâ”€â”€ optimizer.pt
        â”œâ”€â”€ scheduler.pt
        â”œâ”€â”€ rng_state.pth
        â””â”€â”€ training_args.bin
    â”œâ”€â”€ checkpoint-version2
    â””â”€â”€ ...
â”œâ”€â”€ data_scripts
    â”œâ”€â”€ windows
    â””â”€â”€ unix
â”œâ”€â”€ result
    â”œâ”€â”€ plots
    â””â”€â”€ tables
â”œâ”€â”€ category_samples
    â”œâ”€â”€ DRAW
    â”œâ”€â”€ DRAW_L
    â””â”€â”€ ...
â”œâ”€â”€ run.py
â”œâ”€â”€ classifier.py
â”œâ”€â”€ utils.py
â””â”€â”€ ...
</code></pre>
</details>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>The <code class="notranslate">movel_&lt;revision&gt;</code> folder naming is generated from the HF ğŸ˜Š repo <sup><a href="#user-content-fn-1-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-1-7-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”— <code class="notranslate">revision</code> value and does <strong>NOT</strong><br>
affect the trained model naming, other training parameters do.<br>
Since the length of the dataloader depends not only on the size of the dataset but also on the preset batch size,<br>
and test subset ratio.</p>
</div>
<p>You can slightly change the <code class="notranslate">test_size</code> and / or<br>
the <code class="notranslate">batch</code> variable value in the <a href="config.txt">config.txt</a> âš™ file to train a differently named model on the same dataset.<br>
Alternatively, adjust the <strong>model naming generation</strong> in the <a href="classifier.py">classifier.py</a>'s ğŸ“ training function.</p>
<h3>Evaluation ğŸ†</h3>
<p>After the fine-tuned model is saved ğŸ’¾, you can explicitly call for evaluation of the model to get a table of TOP-N classes for<br>
the randomly composed subset (10% in size by default) of the training page folder.</p>
<p>There is an option of setting <code class="notranslate">test_size</code> to 0.8 and use all the sorted by category pages provided<br>
in <code class="notranslate">[TRAIN]</code>'s folder for evaluation, but do <strong>NOT</strong> launch it on the whole training data you have actually used up<br>
for the evaluated model training.</p>
<p>To do this in the unchanged configuration âš™, automatically create a<br>
confusion matrix plot ğŸ“Š and additionally get raw class probabilities table run:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --eval --raw
</code></pre>
<p><strong>OR</strong> when you don't remember the specific <code class="notranslate">[SETUP]</code> and <code class="notranslate">[TRAIN]</code> variables' values for the trained model, you can use:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --eval -m './model/model_&lt;your_model_number_code&gt;'
</code></pre>
<p>Finally, when your model is trained and you are happy with its performance tests, you can uncomment a code line<br>
in the <a href="run.py">run.py</a> ğŸ“ file for <strong>HF ğŸ˜Š hub model push</strong>. This functionality has already been implemented and can be<br>
accessed through the <code class="notranslate">--hf</code> flag using the values set in the <code class="notranslate">[HF]</code> section for the <code class="notranslate">token</code> and <code class="notranslate">repo_name</code> variables.</p>
<p>In this case, you must <strong>rename the trained model folder</strong> in respect to the <code class="notranslate">revision</code> value (dots in the naming are skipped, e.g.<br>
revision <code class="notranslate">v1.9.22</code> turns to <code class="notranslate">model_v1922</code> model folder), and only then run repo push.</p>
<div class="markdown-alert markdown-alert-caution"><p class="markdown-alert-title"><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>Set your own <code class="notranslate">repo_name</code> to the empty one of yours on HF ğŸ˜Š hub, then in the <strong>Settings</strong> of your HF ğŸ˜Š account<br>
find the <strong>Access Tokens</strong> section and generate a new token - copy and paste its value to the <code class="notranslate">token</code> variable. Before committing<br>
those <a href="config.txt">config.txt</a> âš™ file changes via git replace the full <code class="notranslate">token</code> value with its shortened version for security reasons.</p>
</div>
<hr>
<h2>Contacts ğŸ“§</h2>
<p><strong>For support write to:</strong> <a href="mailto:lutsai.k@gmail.com">lutsai.k@gmail.com</a> responsible for this GitHub repository <sup><a href="#user-content-fn-8-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-8-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup> ğŸ”—</p>
<blockquote>
<p>Information about the authors of this project, including their names and ORCIDs, can<br>
be found in the <a href="CITATION.cff">CITATION.cff</a> ğŸ“ file.</p>
</blockquote>
<h2>Acknowledgements ğŸ™</h2>
<ul>
<li><strong>Developed by</strong> UFAL <sup><a href="#user-content-fn-7-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-7-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup> ğŸ‘¥</li>
<li><strong>Funded by</strong> ATRIUM <sup><a href="#user-content-fn-4-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-4-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup>  ğŸ’°</li>
<li><strong>Shared by</strong> ATRIUM <sup><a href="#user-content-fn-4-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-4-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup> &amp; UFAL <sup><a href="#user-content-fn-7-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-7-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup> ğŸ”—</li>
<li><strong>Model type:</strong> fine-tuned ViT with a 224x224 <sup><a href="#user-content-fn-2-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-2-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> ğŸ”— or 384x384 <sup><a href="#user-content-fn-13-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-13-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> <sup><a href="#user-content-fn-14-797c52fbacb9b3587219470fbc8b4081" id="user-content-fnref-14-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> ğŸ”— resolution size</li>
</ul>
<p><strong>Â©ï¸ 2022 UFAL &amp; ATRIUM</strong></p>
<hr>
<h2>Appendix ğŸ¤“</h2>
<details>
<summary>README emoji codes ğŸ‘€</summary>
<ul>
<li>ğŸ–¥ - your computer</li>
<li>ğŸª§ - label/category/class</li>
<li>ğŸ“„ - page/file</li>
<li>ğŸ“ - folder/directory</li>
<li>ğŸ“Š - generated diagrams or plots</li>
<li>ğŸŒ³ - tree of file structure</li>
<li>âŒ› - time-consuming process</li>
<li>âœï¸ - manual action</li>
<li>ğŸ† - performance measurement</li>
<li>ğŸ˜Š - Hugging Face (HF)</li>
<li>ğŸ“§ - contacts</li>
<li>ğŸ‘€ - click to see</li>
<li>âš™ï¸ - configuration/settings</li>
<li>ğŸ“ - link to the internal file</li>
<li>ğŸ”— - link to the external website</li>
</ul>
</details>
<details>
<summary>Content specific emoji codes ğŸ‘€</summary>
<ul>
<li>ğŸ“ - table content</li>
<li>ğŸ“ˆ - drawings/paintings/diagrams</li>
<li>ğŸŒ„ - photos</li>
<li>âœï¸ - handwritten content</li>
<li>ğŸ“„ - text content</li>
<li>ğŸ“° - mixed types of text content, maybe with graphics</li>
</ul>
</details>
<details>
<summary>Decorative emojis ğŸ‘€</summary>
<ul>
<li>ğŸ“‡ğŸ“œğŸ”§â–¶ğŸª„ğŸª›ï¸ğŸ“¦ğŸ”ğŸ“šğŸ™ğŸ‘¥ğŸ“¬ğŸ¤“ - decorative purpose only</li>
</ul>
</details>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p>
    <p>Alternative version of this README file is available in <a href="README.md">README.md</a> ğŸ“ markdown</p>
</div>
<section data-footnotes="" class="footnotes"><h2 id="footnote-label" class="sr-only">Footnotes</h2>
<ol>
<li id="user-content-fn-1-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://huggingface.co/ufal/vit-historical-page">https://huggingface.co/ufal/vit-historical-page</a> <a href="#user-content-fnref-1-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-1-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 1-2" class="data-footnote-backref">â†©<sup>2</sup></a> <a href="#user-content-fnref-1-3-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 1-3" class="data-footnote-backref">â†©<sup>3</sup></a> <a href="#user-content-fnref-1-4-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 1-4" class="data-footnote-backref">â†©<sup>4</sup></a> <a href="#user-content-fnref-1-5-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 1-5" class="data-footnote-backref">â†©<sup>5</sup></a> <a href="#user-content-fnref-1-6-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 1-6" class="data-footnote-backref">â†©<sup>6</sup></a> <a href="#user-content-fnref-1-7-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 1-7" class="data-footnote-backref">â†©<sup>7</sup></a></p>
</li>
<li id="user-content-fn-2-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://huggingface.co/google/vit-base-patch16-224">https://huggingface.co/google/vit-base-patch16-224</a> <a href="#user-content-fnref-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-2-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 2-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-13-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://huggingface.co/google/vit-base-patch16-384">https://huggingface.co/google/vit-base-patch16-384</a> <a href="#user-content-fnref-13-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-13-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 3-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-14-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://huggingface.co/google/vit-large-patch16-384">https://huggingface.co/google/vit-large-patch16-384</a> <a href="#user-content-fnref-14-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-14-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 4-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-10-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://developer.nvidia.com/cuda-python">https://developer.nvidia.com/cuda-python</a> <a href="#user-content-fnref-10-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-10-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 5-2" class="data-footnote-backref">â†©<sup>2</sup></a> <a href="#user-content-fnref-10-3-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 5-3" class="data-footnote-backref">â†©<sup>3</sup></a></p>
</li>
<li id="user-content-fn-3-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://docs.python.org/3/library/venv.html">https://docs.python.org/3/library/venv.html</a> <a href="#user-content-fnref-3-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-5-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://imagemagick.org/script/download.php#windows">https://imagemagick.org/script/download.php#windows</a> <a href="#user-content-fnref-5-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-5-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 7-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-6-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://www.ghostscript.com/releases/gsdnld.html">https://www.ghostscript.com/releases/gsdnld.html</a> <a href="#user-content-fnref-6-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 8" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-9-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://www.openoffice.org/download/">https://www.openoffice.org/download/</a> <a href="#user-content-fnref-9-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 9" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-11-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments">https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments</a> <a href="#user-content-fnref-11-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 10" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-12-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://pytorch.org/vision/0.20/transforms.html">https://pytorch.org/vision/0.20/transforms.html</a> <a href="#user-content-fnref-12-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 11" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-12-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 11-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-8-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://github.com/ufal/atrium-page-classification">https://github.com/ufal/atrium-page-classification</a> <a href="#user-content-fnref-8-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 12" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-7-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://ufal.mff.cuni.cz/home-page">https://ufal.mff.cuni.cz/home-page</a> <a href="#user-content-fnref-7-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 13" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-7-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 13-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-4-797c52fbacb9b3587219470fbc8b4081">
<p><a href="https://atrium-research.eu/">https://atrium-research.eu/</a> <a href="#user-content-fnref-4-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 14" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-4-2-797c52fbacb9b3587219470fbc8b4081" data-footnote-backref="" aria-label="Back to reference 14-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
</ol>
</section></article>
	</body>
</html>
