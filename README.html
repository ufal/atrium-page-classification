<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Image classification using fine-tuned CLIP - for historical document sorting</title>
		<style>.markdown-body{color-scheme:dark;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;margin:0;color:#f0f6fc;background-color:#0d1117;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","Noto Sans",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji";font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body .octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}.markdown-body h1:hover .anchor .octicon-link:before,.markdown-body h2:hover .anchor .octicon-link:before,.markdown-body h3:hover .anchor .octicon-link:before,.markdown-body h4:hover .anchor .octicon-link:before,.markdown-body h5:hover .anchor .octicon-link:before,.markdown-body h6:hover .anchor .octicon-link:before{width:16px;height:16px;content:' ';display:inline-block;background-color:currentColor;-webkit-mask-image:url("data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' version='1.1' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg>");mask-image:url("data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' version='1.1' aria-hidden='true'><path fill-rule='evenodd' d='M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z'></path></svg>")}.markdown-body details,.markdown-body figcaption,.markdown-body figure{display:block}.markdown-body summary{display:list-item}.markdown-body [hidden]{display:none!important}.markdown-body a{background-color:transparent;color:#4493f8;text-decoration:none}.markdown-body abbr[title]{border-bottom:none;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}.markdown-body b,.markdown-body strong{font-weight:600}.markdown-body dfn{font-style:italic}.markdown-body h1{margin:.67em 0;font-weight:600;padding-bottom:.3em;font-size:2em;border-bottom:1px solid #3d444db3}.markdown-body mark{background-color:#bb800926;color:#f0f6fc}.markdown-body small{font-size:90%}.markdown-body sub,.markdown-body sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}.markdown-body sub{bottom:-.25em}.markdown-body sup{top:-.5em}.markdown-body img{border-style:none;max-width:100%;box-sizing:content-box}.markdown-body code,.markdown-body kbd,.markdown-body pre,.markdown-body samp{font-family:monospace;font-size:1em}.markdown-body figure{margin:1em 2.5rem}.markdown-body hr{box-sizing:content-box;overflow:hidden;background:0 0;border-bottom:1px solid #3d444db3;height:.25em;padding:0;margin:1.5rem 0;background-color:#3d444d;border:0}.markdown-body input{font:inherit;margin:0;overflow:visible;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=button],.markdown-body [type=reset],.markdown-body [type=submit]{-webkit-appearance:button;appearance:button}.markdown-body [type=checkbox],.markdown-body [type=radio]{box-sizing:border-box;padding:0}.markdown-body [type=number]::-webkit-inner-spin-button,.markdown-body [type=number]::-webkit-outer-spin-button{height:auto}.markdown-body [type=search]::-webkit-search-cancel-button,.markdown-body [type=search]::-webkit-search-decoration{-webkit-appearance:none;appearance:none}.markdown-body ::-webkit-input-placeholder{color:inherit;opacity:.54}.markdown-body ::-webkit-file-upload-button{-webkit-appearance:button;appearance:button;font:inherit}.markdown-body a:hover{text-decoration:underline}.markdown-body ::placeholder{color:#9198a1;opacity:1}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body table{border-spacing:0;border-collapse:collapse;display:block;width:max-content;max-width:100%;overflow:auto;font-variant:tabular-nums}.markdown-body td,.markdown-body th{padding:0}.markdown-body details summary{cursor:pointer}.markdown-body [role=button]:focus,.markdown-body a:focus,.markdown-body input[type=checkbox]:focus,.markdown-body input[type=radio]:focus{outline:2px solid #1f6feb;outline-offset:-2px;box-shadow:none}.markdown-body [role=button]:focus:not(:focus-visible),.markdown-body a:focus:not(:focus-visible),.markdown-body input[type=checkbox]:focus:not(:focus-visible),.markdown-body input[type=radio]:focus:not(:focus-visible){outline:solid 1px transparent}.markdown-body [role=button]:focus-visible,.markdown-body a:focus-visible,.markdown-body input[type=checkbox]:focus-visible,.markdown-body input[type=radio]:focus-visible{outline:2px solid #1f6feb;outline-offset:-2px;box-shadow:none}.markdown-body a:not([class]):focus,.markdown-body a:not([class]):focus-visible,.markdown-body input[type=checkbox]:focus,.markdown-body input[type=checkbox]:focus-visible,.markdown-body input[type=radio]:focus,.markdown-body input[type=radio]:focus-visible{outline-offset:0}.markdown-body kbd{display:inline-block;padding:.25rem;font:11px ui-monospace,SFMono-Regular,SF Mono,Menlo,Consolas,Liberation Mono,monospace;line-height:10px;color:#f0f6fc;vertical-align:middle;background-color:#151b23;border:solid 1px #3d444db3;border-bottom-color:#3d444db3;border-radius:6px;box-shadow:inset 0 -1px 0 #3d444db3}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:1.5rem;margin-bottom:1rem;font-weight:600;line-height:1.25}.markdown-body h2{font-weight:600;padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #3d444db3}.markdown-body h3{font-weight:600;font-size:1.25em}.markdown-body h4{font-weight:600;font-size:1em}.markdown-body h5{font-weight:600;font-size:.875em}.markdown-body h6{font-weight:600;font-size:.85em;color:#9198a1}.markdown-body p{margin-top:0;margin-bottom:10px}.markdown-body blockquote{margin:0;padding:0 1em;color:#9198a1;border-left:.25em solid #3d444d}.markdown-body ol,.markdown-body ul{margin-top:0;margin-bottom:0;padding-left:2em}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body dd{margin-left:0}.markdown-body code,.markdown-body samp,.markdown-body tt{font-family:ui-monospace,SFMono-Regular,SF Mono,Menlo,Consolas,Liberation Mono,monospace;font-size:12px}.markdown-body pre{margin-top:0;margin-bottom:0;font-family:ui-monospace,SFMono-Regular,SF Mono,Menlo,Consolas,Liberation Mono,monospace;font-size:12px;word-wrap:normal}.markdown-body .octicon{display:inline-block;overflow:visible!important;vertical-align:text-bottom;fill:currentColor}.markdown-body input::-webkit-inner-spin-button,.markdown-body input::-webkit-outer-spin-button{margin:0;appearance:none}.markdown-body .mr-2{margin-right:.5rem!important}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#f85149}.markdown-body .anchor{float:left;padding-right:.25rem;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:0}.markdown-body blockquote,.markdown-body details,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:1rem}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#f0f6fc;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{padding:0 .2em;font-size:inherit}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{padding-bottom:0;border-bottom:0}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol[type="a s"]{list-style-type:lower-alpha}.markdown-body ol[type="A s"]{list-style-type:upper-alpha}.markdown-body ol[type="i s"]{list-style-type:lower-roman}.markdown-body ol[type="I s"]{list-style-type:upper-roman}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:1rem}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:1rem;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 1rem;margin-bottom:1rem}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #3d444d}.markdown-body table td>:last-child{margin-bottom:0}.markdown-body table tr{background-color:#0d1117;border-top:1px solid #3d444db3}.markdown-body table tr:nth-child(2n){background-color:#151b23}.markdown-body table img{background-color:transparent}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #3d444d}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#f0f6fc}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:.2em .4em;margin:0;font-size:85%;white-space:break-spaces;background-color:#656c7633;border-radius:6px}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre code{font-size:100%}.markdown-body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:1rem}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:1rem;overflow:auto;font-size:85%;line-height:1.45;color:#f0f6fc;background-color:#151b23;border-radius:6px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-num{padding:10px .5rem 9px;text-align:right;background:#0d1117;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:600;background:#151b23;border-top:0}.markdown-body [data-footnote-ref]::before{content:"["}.markdown-body [data-footnote-ref]::after{content:"]"}.markdown-body .footnotes{font-size:12px;color:#9198a1;border-top:1px solid #3d444d}.markdown-body .footnotes ol{padding-left:1rem}.markdown-body .footnotes ol ul{display:inline-block;padding-left:1rem;margin-top:1rem}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target::before{position:absolute;top:calc(.5rem*-1);right:calc(.5rem*-1);bottom:calc(.5rem*-1);left:calc(1.5rem*-1);pointer-events:none;content:"";border:2px solid #1f6feb;border-radius:6px}.markdown-body .footnotes li:target{color:#f0f6fc}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body body:has(:modal){padding-right:var(--dialog-scrollgutter)!important}.markdown-body .pl-c{color:#9198a1}.markdown-body .pl-c1,.markdown-body .pl-s .pl-v{color:#79c0ff}.markdown-body .pl-e,.markdown-body .pl-en{color:#d2a8ff}.markdown-body .pl-s .pl-s1,.markdown-body .pl-smi{color:#f0f6fc}.markdown-body .pl-ent{color:#7ee787}.markdown-body .pl-k{color:#ff7b72}.markdown-body .pl-pds,.markdown-body .pl-s,.markdown-body .pl-s .pl-pse .pl-s1,.markdown-body .pl-sr,.markdown-body .pl-sr .pl-cce,.markdown-body .pl-sr .pl-sra,.markdown-body .pl-sr .pl-sre{color:#a5d6ff}.markdown-body .pl-smw,.markdown-body .pl-v{color:#ffa657}.markdown-body .pl-bu{color:#f85149}.markdown-body .pl-ii{color:#f0f6fc;background-color:#8e1519}.markdown-body .pl-c2{color:#f0f6fc;background-color:#b62324}.markdown-body .pl-sr .pl-cce{font-weight:700;color:#7ee787}.markdown-body .pl-ml{color:#f2cc60}.markdown-body .pl-mh,.markdown-body .pl-mh .pl-en,.markdown-body .pl-ms{font-weight:700;color:#1f6feb}.markdown-body .pl-mi{font-style:italic;color:#f0f6fc}.markdown-body .pl-mb{font-weight:700;color:#f0f6fc}.markdown-body .pl-md{color:#ffdcd7;background-color:#67060c}.markdown-body .pl-mi1{color:#aff5b4;background-color:#033a16}.markdown-body .pl-mc{color:#ffdfb6;background-color:#5a1e02}.markdown-body .pl-mi2{color:#f0f6fc;background-color:#1158c7}.markdown-body .pl-mdr{font-weight:700;color:#d2a8ff}.markdown-body .pl-ba{color:#9198a1}.markdown-body .pl-sg{color:#3d444d}.markdown-body .pl-corl{text-decoration:underline;color:#a5d6ff}.markdown-body [role=button]:focus:not(:focus-visible),.markdown-body [role=tabpanel][tabindex="0"]:focus:not(:focus-visible),.markdown-body a:focus:not(:focus-visible),.markdown-body button:focus:not(:focus-visible),.markdown-body summary:focus:not(:focus-visible){outline:0;box-shadow:none}.markdown-body [tabindex="0"]:focus:not(:focus-visible),.markdown-body details-dialog:focus:not(:focus-visible){outline:0}.markdown-body g-emoji{display:inline-block;min-width:1ch;font-family:"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:1em;font-style:normal!important;font-weight:400;line-height:1;vertical-align:-.075em}.markdown-body g-emoji img{width:1em;height:1em}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item label{font-weight:400}.markdown-body .task-list-item.enabled label{cursor:pointer}.markdown-body .task-list-item+.task-list-item{margin-top:.25rem}.markdown-body .task-list-item .handle{display:none}.markdown-body .task-list-item-checkbox{margin:0 .2em .25em -1.4em;vertical-align:middle}.markdown-body ul:dir(rtl) .task-list-item-checkbox{margin:0 -1.6em .25em .2em}.markdown-body ol:dir(rtl) .task-list-item-checkbox{margin:0 -1.6em .25em .2em}.markdown-body .contains-task-list:focus-within .task-list-item-convert-container,.markdown-body .contains-task-list:hover .task-list-item-convert-container{display:block;width:auto;height:24px;overflow:visible;clip:auto}.markdown-body ::-webkit-calendar-picker-indicator{filter:invert(50%)}.markdown-body .markdown-alert{padding:.5rem 1rem;margin-bottom:1rem;color:inherit;border-left:.25em solid #3d444d}.markdown-body .markdown-alert>:first-child{margin-top:0}.markdown-body .markdown-alert>:last-child{margin-bottom:0}.markdown-body .markdown-alert .markdown-alert-title{display:flex;font-weight:500;align-items:center;line-height:1}.markdown-body .markdown-alert.markdown-alert-note{border-left-color:#1f6feb}.markdown-body .markdown-alert.markdown-alert-note .markdown-alert-title{color:#4493f8}.markdown-body .markdown-alert.markdown-alert-important{border-left-color:#8957e5}.markdown-body .markdown-alert.markdown-alert-important .markdown-alert-title{color:#ab7df8}.markdown-body .markdown-alert.markdown-alert-warning{border-left-color:#9e6a03}.markdown-body .markdown-alert.markdown-alert-warning .markdown-alert-title{color:#d29922}.markdown-body .markdown-alert.markdown-alert-tip{border-left-color:#238636}.markdown-body .markdown-alert.markdown-alert-tip .markdown-alert-title{color:#3fb950}.markdown-body .markdown-alert.markdown-alert-caution{border-left-color:#da3633}.markdown-body .markdown-alert.markdown-alert-caution .markdown-alert-title{color:#f85149}.markdown-body>:first-child>.heading-element:first-child{margin-top:0!important}.markdown-body .highlight pre:has(+.zeroclipboard-container){min-height:52px}</style>
		<style>
			body {
				margin: 0;
			}

			.markdown-body-content {
				box-sizing: border-box;
				min-width: 200px;
				max-width: 980px;
				margin: 0 auto;
				padding: 45px;
			}

			@media (max-width: 767px) {
				.markdown-body-content {
					padding: 15px;
				}
			}

			.markdown-body {
				--base-size-8: 8px;
				--base-size-16: 16px;
			}
		</style>
	</head>
	<body class="markdown-body">
		<article class="markdown-body-content"><h1>Image classification using fine-tuned CLIP - for historical document sorting</h1>
<h3>Goal: solve a task of archive page images sorting (for their further content-based processing)</h3>
<p><strong>Scope:</strong> Processing of images, training / evaluation of CLIP model,<br>
input file/directory processing, class ğŸª§  (category) results of top<br>
N predictions output, predictions summarizing into a tabular format,<br>
HF ğŸ˜Š hub <sup><a href="#user-content-fn-1-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-1-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”— support for the model, multiplatform (Win/Lin) data<br>
preparation scripts for PDF to PNG conversion</p>
<h3>Table of contents ğŸ“‘</h3>
<ul>
<li><a href="#versions-">Versions ğŸ</a></li>
<li><a href="#model-description-">Model description ğŸ“‡</a>
<ul>
<li><a href="#data-">Data ğŸ“œ</a></li>
<li><a href="#categories-">Categories ğŸª§ï¸</a></li>
</ul>
</li>
<li><a href="#how-to-install-">How to install ğŸ”§</a></li>
<li><a href="#how-to-run-prediction--modes">How to run prediction ğŸª„ modes</a>
<ul>
<li><a href="#page-processing-">Page processing ğŸ“„</a></li>
<li><a href="#directory-processing-">Directory processing ğŸ“</a></li>
</ul>
</li>
<li><a href="#results-">Results ğŸ“Š</a>
<ul>
<li><a href="#result-tables-and-their-columns-">Result tables and their columns ğŸ“ğŸ“‹</a></li>
</ul>
</li>
<li><a href="#data-preparation-">Data preparation ğŸ“¦</a>
<ul>
<li><a href="#pdf-to-png-">PDF to PNG ğŸ“š</a></li>
<li><a href="#png-pages-annotation-">PNG pages annotation ğŸ”</a></li>
<li><a href="#png-pages-sorting-for-training-">PNG pages sorting for training ğŸ“¬</a></li>
</ul>
</li>
<li><a href="#for-developers-">For developers ğŸª›</a>
<ul>
<li><a href="#training-">Training ğŸ’ª</a></li>
<li><a href="#evaluation-">Evaluation ğŸ†</a></li>
</ul>
</li>
<li><a href="#contacts-">Contacts ğŸ“§</a></li>
<li><a href="#acknowledgements-">Acknowledgements ğŸ™</a></li>
<li><a href="#appendix-">Appendix ğŸ¤“</a></li>
</ul>
<hr>
<h2>Versions ğŸ</h2>
<p>There are currently 4 version of the model available for download, both of them have the same set of categories,<br>
but different data annotations. The latest <code class="notranslate">v1.1.3.7</code> is considered to be default and can be found in the <code class="notranslate">main</code> branch<br>
of HF ğŸ˜Š hub <sup><a href="#user-content-fn-1-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-1-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”—</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th align="right">Version</th>
<th>Base code</th>
<th align="center">Pages</th>
<th align="center">PDFs</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code class="notranslate">v1.1</code></td>
<td><code class="notranslate">ViT-B/16</code></td>
<td align="center">14270</td>
<td align="center"><strong>5730</strong></td>
<td align="left">smallest (old default)</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v1.2</code></td>
<td><code class="notranslate">ViT-B/32</code></td>
<td align="center">14270</td>
<td align="center"><strong>5730</strong></td>
<td align="left">small with higher granularity</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v2.1</code></td>
<td><code class="notranslate">ViT-L/14</code></td>
<td align="center">14270</td>
<td align="center"><strong>5730</strong></td>
<td align="left">large</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v2.2</code></td>
<td><code class="notranslate">ViT-L/14@336</code></td>
<td align="center">14270</td>
<td align="center"><strong>5730</strong></td>
<td align="left">large with highest resolution</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v1.1.3.[1,3,4,6,7]</code></td>
<td><code class="notranslate">ViT-B/16</code></td>
<td align="center">38625</td>
<td align="center"><strong>37328</strong></td>
<td align="left">smallest and most accurate</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v1.2.3</code></td>
<td><code class="notranslate">ViT-B/32</code></td>
<td align="center">38625</td>
<td align="center"><strong>37328</strong></td>
<td align="left">small and 2nd in accuracy</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v2.1.3.1</code></td>
<td><code class="notranslate">ViT-B/14</code></td>
<td align="center">38625</td>
<td align="center"><strong>37328</strong></td>
<td align="left">larges and not too accurate</td>
</tr>
<tr>
<td align="right"><code class="notranslate">v2.2.3.4</code></td>
<td><code class="notranslate">ViT-B/14@336</code></td>
<td align="center">38625</td>
<td align="center"><strong>37328</strong></td>
<td align="left">larges and not too accurate</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<details>
<summary>Base model - size ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th><strong>Version</strong></th>
<th><strong>Disk space</strong></th>
<th><strong>Parameters (Millions)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="notranslate">openai/clip-vit-base-patch16</code></td>
<td>992 Mb</td>
<td>149.62 M</td>
</tr>
<tr>
<td><code class="notranslate">openai/clip-vit-base-patch32</code></td>
<td>1008 Mb</td>
<td>151.28 M</td>
</tr>
<tr>
<td><code class="notranslate">openai/clip-vit-large-patch14</code></td>
<td>1.5 Gb</td>
<td>427.62 M</td>
</tr>
<tr>
<td><code class="notranslate">openai/clip-vit-large-patch14-336</code></td>
<td>1.5 Gb</td>
<td>427.94 M</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<h2>Model description ğŸ“‡</h2>
<p><a target="_blank" rel="noopener noreferrer" href="architecture_clip.png"><img src="architecture_clip.png" alt="architecture_diagram" style="max-width: 100%;"></a></p>
<p>ğŸ”² <strong>Fine-tuned</strong> model repository: UFAL's <strong>clip-historical-page</strong> <sup><a href="#user-content-fn-1-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-1-3-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”—</p>
<p>ğŸ”³ <strong>Base</strong> model repository: OpenAI's <strong>clip-vit-base-patch16</strong>,  <strong>clip-vit-base-patch32</strong>,  <strong>clip-vit-large-patch14</strong>, <strong>clip-vit-large-patch14-336</strong> <sup><a href="#user-content-fn-2-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> <sup><a href="#user-content-fn-13-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-13-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> <sup><a href="#user-content-fn-14-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-14-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> <sup><a href="#user-content-fn-15-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-15-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> ğŸ”—</p>
<p>The model was trained on the manually âœï¸ annotated dataset of historical documents, in particular, images of pages<br>
from the archival documents with paper sources that were scanned into digital form.</p>
<p>The images contain various combinations of texts ï¸ğŸ“„, tables ğŸ“, drawings ğŸ“ˆ, and photos ğŸŒ„ -<br>
categories ğŸª§ described <a href="#categories-">below</a> were formed based on those archival documents. Page examples can be found in<br>
the <a href="category_samples">category_samples</a> ğŸ“ directory.</p>
<p>The key <strong>use case</strong> of the provided model and data processing pipeline is to classify an input PNG image from PDF scanned<br>
paper source into one of the categories - each responsible for the following content-specific data processing pipeline.</p>
<blockquote>
<p>In other words, when several APIs for different OCR subtasks are at your disposal - run this classifier first to<br>
mark the input data as machine-typed (old style fonts) / handwritten âœï¸ / just printed plain ï¸ğŸ“„ text<br>
or structured in tabular ğŸ“ format text, as well as to mark the presence of the printed ğŸŒ„ or drawn ğŸ“ˆ graphic<br>
materials yet to be extracted from the page images.</p>
</blockquote>
<p><a target="_blank" rel="noopener noreferrer" href="model_acc_compared.png"><img src="model_acc_compared.png" alt="comparison_graph.png" style="max-width: 100%;"></a></p>
<p>The figure above shows accuracy and parameters comparison of different base models tested on the same data, demonstrating<br>
best models overall (above the trendline) which mainly includes image-based models like CNNs and transformers, and the hybrid<br>
CLIP models themselves (best versions of each base model).</p>
<p>Versions of CLIP models are grounded on the textual category description sets, all illustrated in<br>
<a href="result%2Fstats%2Fmodel_accuracy_plot.png">descriptions_comparison_graph.png</a> ğŸ“ which is a graph containing separate and averaged results<br>
of all category ğŸª§ descriptions.</p>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fstats%2Fmodel_accuracy_plot_zero.png"><img src="result%2Fstats%2Fmodel_accuracy_plot_zero.png" alt="model_accuracy_plot_zero.png" style="max-width: 100%;"></a></p>
<p>As our experiments showed, the averaging strategy is not the best. Moreover, the smallest model<br>
ViT-B/16 showed the best results after fine-tuning model on some selected category ğŸª§ set like<br>
<code class="notranslate">mid</code> which as well as <code class="notranslate">avg</code> scored above average in all 4 base models.</p>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fstats%2Fmodel_accuracy_plot.png"><img src="result%2Fstats%2Fmodel_accuracy_plot.png" alt="description comparison graph" style="max-width: 100%;"></a></p>
<p>Check out all of the prepared category ğŸª§ descriptions in the <a href="category_descriptions">category_descriptions</a> ğŸ“ folder.<br>
Which supports versions mapping from 1 to 9 for the csv files starting with <code class="notranslate">page_categories_</code> prefix. The separate set<br>
starting with <code class="notranslate">TOTAL</code> is a mixture of all category descriptions, a set starting with <code class="notranslate">GENERAL</code> is a simplified category ğŸª§ set<br>
(only 4 classes), and a set starting with <code class="notranslate">EXPANDED</code> is an experimental more fine-grained in categories version of the category ğŸª§ descriptions.</p>
<h3>Data ğŸ“œ</h3>
<p>The dataset is provided under Public Domain license, and consists of <strong>48,499</strong> PNG images of pages from <strong>37,328</strong> archival documents.<br>
The source image files and their annotation can be found in the LINDAT repository <sup><a href="#user-content-fn-16-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-16-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup> ğŸ”—.</p>
<p>The annotation provided includes 5 different<br>
dataset splits of <code class="notranslate">vX.X.3</code> model versions, and it's recommended to average all 5 trained model weights to get a more robust<br>
model for prediction (in some cases, like <code class="notranslate">TEXT</code> and <code class="notranslate">TEXT_T</code> categories which samples very often look the same, the accuracy of those<br>
problematic categories could drop below 90% with off-diagonal errors rising above 10% after the averaging of trained models). Anyhow, the<br>
averaged model usually score higher accuracy than any of its individual components... or sometimes causes a drop in accuracy for<br>
the most ambiguous categories ğŸª§ï¸ - depends mostly on the base model choice.</p>
<p>Our dataset is not split using a simple random shuffle. This is because the data contains structured and clustered<br>
distributions of page types within many categories. A random shuffle would likely result in subsets with poor<br>
representative variability.</p>
<p>Instead, we use a deterministic, periodic sampling method with a randomized offset. To maximize the size of the<br>
training ğŸ’ª set, we select the development and test ğŸ† subsets first. The training subset then consists of all remaining pages.</p>
<p>Here's the per-category ğŸª§ procedure for selecting the development and test ğŸ† sets:</p>
<ol>
<li>For the category of size <code class="notranslate">N</code> compute  the desired subset size, <code class="notranslate">k</code>, as a fixed proportion (<code class="notranslate">test_ratio</code> which was 10%) of <code class="notranslate">N</code></li>
<li>Compute a selection step, <code class="notranslate">S</code>, as <code class="notranslate">S â‰ˆ N/k</code> which serves a period base for the selection</li>
<li>Apply a random shift to <code class="notranslate">S</code> - an integer index in the range <code class="notranslate">[S_i - S/4; S_i + S/4]</code> for every <code class="notranslate">i</code>-th of <code class="notranslate">k</code> steps of <code class="notranslate">S</code>.</li>
<li>Select every <code class="notranslate">S</code>-th (<code class="notranslate">S</code>-thish in fact) element from the alphabetically ordered sequence after applying the random shift.</li>
<li>Finally, Limit selected indices to be within the range of the category size <code class="notranslate">N</code>.</li>
</ol>
<p>This method produces subsets that:</p>
<ul>
<li>Respect the original ordering and local clustering in the data</li>
<li>Preserve the proportional representation of each category</li>
<li>Introduce controlled randomness, so the selected samples are not strictly periodic</li>
</ul>
<p>This ensures that our subsets cover the full chronological and structural variability of the<br>
collection, leading to a more robust and reliable model evaluation.</p>
<p><strong>Training</strong> ğŸ’ª set of the model: <strong>14270</strong> images for <code class="notranslate">vX.X</code></p>
<p><strong>Training</strong> ğŸ’ª set of the model: <strong>38625</strong> images for <code class="notranslate">vX.X.3</code></p>
<p>The training subsets above are followed by the test sets below:</p>
<p><strong>Evaluation</strong> ğŸ† set:  <strong>1290</strong> images for <code class="notranslate">vX.X</code> models</p>
<p><strong>Evaluation</strong> ğŸ† set:  <strong>4823</strong> images (for <code class="notranslate">vX.X.3</code> models)</p>
<p>Manual âœï¸ annotation was performed beforehand and took some time âŒ›, the categories ğŸª§ tabulated  <a href="#categories-">below</a> were formed from<br>
different sources of the archival documents originated in the 1920-2020 years span.</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>Category</th>
<th>Dataset 0</th>
<th>Dataset 1</th>
<th>Dataset 2</th>
<th>Dataset 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>DRAW</td>
<td>1090 (9.1%)</td>
<td>1368 (8.8%)</td>
<td>1472 (9.3%)</td>
<td>2709 (5.6%)</td>
</tr>
<tr>
<td>DRAW_L</td>
<td>1091 (9.1%)</td>
<td>1383 (8.9%)</td>
<td>1402 (8.8%)</td>
<td>2921 (6.0%)</td>
</tr>
<tr>
<td>LINE_HW</td>
<td>1055 (8.8%)</td>
<td>1113 (7.2%)</td>
<td>1115 (7.0%)</td>
<td>2514 (5.2%)</td>
</tr>
<tr>
<td>LINE_P</td>
<td>1092 (9.1%)</td>
<td>1540 (9.9%)</td>
<td>1580 (10.0%)</td>
<td>2439 (5.0%)</td>
</tr>
<tr>
<td>LINE_T</td>
<td>1098 (9.2%)</td>
<td>1664 (10.7%)</td>
<td>1668 (10.5%)</td>
<td>9883 (20.4%)</td>
</tr>
<tr>
<td>PHOTO</td>
<td>1081 (9.1%)</td>
<td>1632 (10.5%)</td>
<td>1730 (10.9%)</td>
<td>2691 (5.5%)</td>
</tr>
<tr>
<td>PHOTO_L</td>
<td>1087 (9.1%)</td>
<td>1087 (7.0%)</td>
<td>1088 (6.9%)</td>
<td>2830 (5.8%)</td>
</tr>
<tr>
<td>TEXT</td>
<td>1091 (9.1%)</td>
<td>1587 (10.3%)</td>
<td>1592 (10.0%)</td>
<td>14227 (29.3%)</td>
</tr>
<tr>
<td>TEXT_HW</td>
<td>1091 (9.1%)</td>
<td>1092 (7.1%)</td>
<td>1092 (6.9%)</td>
<td>2008 (4.1%)</td>
</tr>
<tr>
<td>TEXT_P</td>
<td>1083 (9.1%)</td>
<td>1540 (9.9%)</td>
<td>1633 (10.3%)</td>
<td>2312 (4.8%)</td>
</tr>
<tr>
<td>TEXT_T</td>
<td>1081 (9.1%)</td>
<td>1476 (9.5%)</td>
<td>1482 (9.3%)</td>
<td>3965 (8.2%)</td>
</tr>
<tr>
<td><strong>Unique PDFs</strong></td>
<td>5001</td>
<td>5694</td>
<td>5729</td>
<td>37328</td>
</tr>
<tr>
<td><strong>Total Pages</strong></td>
<td>11,940</td>
<td>15,482</td>
<td>15,854</td>
<td>48,499</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p>The table above shows category distribution for different model versions, where the last column<br>
(<code class="notranslate">Dataset 3</code>) corresponds to the latest <code class="notranslate">vX.X.3</code> models data, which actually used 14,000 pages of<br>
<code class="notranslate">TEXT</code> category, while other columns cover all the used samples - specifically 80% as training ğŸ’ª,<br>
and 10% each as development and test ğŸ† sets. The early model version used 90% of the data as training ğŸ’ª<br>
and the remaining 10% as both development and test ğŸ† set due to the lack of annotated (manually<br>
classified) pages.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>Disproportion of the categories ğŸª§ in both training data and provided evaluation <a href="category_samples">category_samples</a> ğŸ“ is<br>
<strong>NOT</strong> intentional, but rather a result of the source data nature.</p>
</div>
<p>The specific content and language of the<br>
source data is irrelevant considering the model's vision resolution, however, all of the data samples were from <strong>archaeological<br>
reports</strong> which may somehow affect the drawing detection preferences due to the common form of objects being ceramic pieces,<br>
arrowheads, and rocks formerly drawn by hand and later illustrated with digital tools (examples can be found in<br>
<a href="category_samples%2FDRAW">category_samples/DRAW</a> ğŸ“)</p>
<p><a target="_blank" rel="noopener noreferrer" href="dataset_timeline.png"><img src="dataset_timeline.png" alt="data_timeline.png" style="max-width: 100%;"></a></p>
<h3>Categories ğŸª§</h3>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th align="right">Labelï¸</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right"><code class="notranslate">DRAW</code></td>
<td align="left"><strong>ğŸ“ˆ - drawings, maps, paintings, schematics, or graphics, potentially containing some text labels or captions</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">DRAW_L</code></td>
<td align="left"><strong>ğŸ“ˆğŸ“ - drawings, etc but presented within a table-like layout or includes a legend formatted as a table</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">LINE_HW</code></td>
<td align="left"><strong>âœï¸ğŸ“ - handwritten text organized in a tabular or form-like structure</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">LINE_P</code></td>
<td align="left"><strong>ğŸ“ - printed text organized in a tabular or form-like structure</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">LINE_T</code></td>
<td align="left"><strong>ğŸ“ - machine-typed text organized in a tabular or form-like structure</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">PHOTO</code></td>
<td align="left"><strong>ğŸŒ„ - photographs or photographic cutouts, potentially with text captions</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">PHOTO_L</code></td>
<td align="left"><strong>ğŸŒ„ğŸ“ - photos presented within a table-like layout or accompanied by tabular annotations</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">TEXT</code></td>
<td align="left"><strong>ğŸ“° - mixtures of printed, handwritten, and/or typed text, potentially with minor graphical elements</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">TEXT_HW</code></td>
<td align="left"><strong>âœï¸ğŸ“„ - only handwritten text in paragraph or block form (non-tabular)</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">TEXT_P</code></td>
<td align="left"><strong>ğŸ“„ - only printed text in paragraph or block form (non-tabular)</strong></td>
</tr>
<tr>
<td align="right"><code class="notranslate">TEXT_T</code></td>
<td align="left"><strong>ğŸ“„ - only machine-typed text in paragraph or block form (non-tabular)</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p>The categories were chosen to sort the pages by the following criteria:</p>
<ul>
<li><strong>presence of graphical elements</strong> (drawings ğŸ“ˆ OR photos ğŸŒ„)</li>
<li><strong>type of text</strong> ğŸ“„ (handwritten âœï¸ï¸ OR printed OR typed OR mixed ğŸ“°)</li>
<li><strong>presence of tabular layout / forms</strong> ğŸ“</li>
</ul>
<blockquote>
<p>The reasons for such distinction are different processing pipelines for different types of pages, which would be<br>
applied after the classification as mentioned <a href="#model-description-">above</a>.</p>
</blockquote>
<p>Examples of pages sorted by category ğŸª§ can be found in the <a href="category_samples">category_samples</a> ğŸ“ directory<br>
which is also available as a testing subset of the training data.</p>
<hr>
<h2>How to install ğŸ”§</h2>
<p>Step-by-step instructions on this program installation are provided here. The easiest way to obtain the model would<br>
be to use the HF ğŸ˜Š hub repository <sup><a href="#user-content-fn-1-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-1-4-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”— that can be easily accessed via this project.</p>
<details>
<summary>Hardware requirements ğŸ‘€</summary>
<p><strong>Minimal</strong> machine ğŸ–¥ï¸ requirements for slow prediction run (and very slow training / evaluation):</p>
<ul>
<li><strong>CPU</strong> with a decent (above average) operational memory size</li>
</ul>
<p><strong>Ideal</strong> machine ğŸ–¥ï¸ requirements for fast prediction (and relatively fast training / evaluation):</p>
<ul>
<li><strong>CPU</strong> of some kind and memory size</li>
<li><strong>GPU</strong> (for real CUDA <sup><a href="#user-content-fn-10-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-10-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup> support - only one of Nvidia's cards)</li>
</ul>
</details>
<div class="markdown-alert markdown-alert-warning"><p class="markdown-alert-title"><svg class="octicon octicon-alert mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Warning</p><p>Make sure you have <strong>Python version 3.10+</strong> installed on your machine ğŸ’» and check its<br>
<strong>hardware requirements</strong> for correct program running provided above.<br>
Then create a separate virtual environment for this project</p>
</div>
<details>
<summary>How to ğŸ‘€</summary>
<p>Clone this project to your local machine ğŸ–¥ï¸ï¸ via:</p>
<pre class="notranslate"><code class="notranslate">cd /local/folder/for/this/project
git init
git clone https://github.com/ufal/atrium-page-classification.git
</code></pre>
<p>Then change to the Vit and EffNet models or CLIP models branch (<code class="notranslate">master</code>, <code class="notranslate">clip</code> or <code class="notranslate">vit</code>):</p>
<pre class="notranslate"><code class="notranslate">cd atrium-page-classification
git checkout vit
</code></pre>
<p><strong>OR</strong> for updating the already cloned project with some changes, go to the folder containing (hidden) <code class="notranslate">.git</code><br>
subdirectory and run pulling which will merge upcoming files with your local changes:</p>
<pre class="notranslate"><code class="notranslate">cd /local/folder/for/this/project/atrium-page-classification
git add &lt;changed_file&gt;
git commit -m 'local changes'
</code></pre>
<p>And then for updating the project with the latest changes from the remote repository, run:</p>
<pre class="notranslate"><code class="notranslate">git pull -X theirs
</code></pre>
<p>Alternatively, if you are interested in a specific branch (<code class="notranslate">master</code>, <code class="notranslate">clip</code> or <code class="notranslate">vit</code>), you can update  it via:</p>
<pre class="notranslate"><code class="notranslate">git fetch origin
git checkout vit        
git pull --ff-only origin vit
</code></pre>
<p>Alternatively, if you do <strong>NOT</strong> care about local changes <strong>OR</strong> you want to get the latest project files,<br>
just remove those files (all <code class="notranslate">.py</code>, <code class="notranslate">.txt</code> and <code class="notranslate">README</code> files) and pull the latest version from the repository:</p>
<pre class="notranslate"><code class="notranslate">cd /local/folder/for/this/project/atrium-page-classification
</code></pre>
<p>And then for a total clean up and update, run:</p>
<pre class="notranslate"><code class="notranslate">rm *.py
rm *.txt
rm README*
git pull
</code></pre>
<p>Alternatively, for a specific branch (<code class="notranslate">master</code>, <code class="notranslate">clip</code> or <code class="notranslate">vit</code>):</p>
<pre class="notranslate"><code class="notranslate">git reset --hard HEAD
git clean -fd
git fetch origin
git checkout vit
git pull origin vit
</code></pre>
<p>Overall, a force update to the remote repository branch (<code class="notranslate">master</code>, <code class="notranslate">clip</code> or <code class="notranslate">vit</code>) looks like this:</p>
<pre class="notranslate"><code class="notranslate">git fetch origin
git checkout vit
git reset --hard origin/vit
</code></pre>
<p>Next step would be a creation of the virtual environment. Follow the <strong>Unix</strong> / <strong>Windows</strong>-specific<br>
instruction at the venv docs <sup><a href="#user-content-fn-3-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-3-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup> ğŸ‘€ğŸ”— if you don't know how to.</p>
<p>After creating the venv folder, activate the environment via:</p>
<pre class="notranslate"><code class="notranslate">source &lt;your_venv_dir&gt;/bin/activate
</code></pre>
<p>and then inside your virtual environment, you should install Python libraries (takes time âŒ›)</p>
</details>
<div class="markdown-alert markdown-alert-caution"><p class="markdown-alert-title"><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>Up to <strong>1 GB of space for model</strong> files and checkpoints is needed, and up to <strong>7 GB<br>
of space for the Python libraries</strong> (Pytorch and its dependencies, etc)</p>
</div>
<p>Installation of Python dependencies can be done via:</p>
<pre class="notranslate"><code class="notranslate">pip install -r requirements.txt
</code></pre>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>The so-called <strong>CUDA <sup><a href="#user-content-fn-10-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-10-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup> support</strong> for Python's PyTorch library is supposed to be automatically installed<br>
at this point - when the presence of the GPU on your machine ğŸ–¥ï¸<br>
is checked for the first time, later it's also checked every time before the model initialization<br>
(for training, evaluation or prediction run).</p>
</div>
<p>After the dependencies installation is finished successfully, in the same virtual environment, you can<br>
run the Python program.</p>
<p>To test that everything works okay and see the flag<br>
descriptions call for <code class="notranslate">--help</code> â“:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py -h
</code></pre>
<p>You should see a (hopefully) helpful message about all available command line flags. Your next step would be<br>
to <strong>pull the model from the HF ğŸ˜Š hub repository <sup><a href="#user-content-fn-1-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-1-5-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”—</strong> via:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --hf
</code></pre>
<p><strong>OR</strong> for specific model version (e.g. <code class="notranslate">main</code>, <code class="notranslate">vX.1</code>, <code class="notranslate">vX.2</code> and <code class="notranslate">vX.X.3</code>) use the <code class="notranslate">--revision</code> flag:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --hf -rev v1.1
</code></pre>
<p><strong>OR</strong> for specific base model version (e.g. <code class="notranslate">ViT-B/16</code>, <code class="notranslate">ViT-B/32</code>, <code class="notranslate">ViT-L/14</code> or <code class="notranslate">ViT-L/14@336px</code>) use the <code class="notranslate">--model</code><br>
or <code class="notranslate">-m</code> flag (only when the trained model version demands such base model as described <a href="#versions-">above</a>):</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --hf -rev v2.2.3 -m `ViT-L/14@336px`
</code></pre>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>If you already have the model files in the <code class="notranslate">models/&lt;base_code&gt;_rev_&lt;revision&gt;</code><br>
directory (e.g., <code class="notranslate">models/ViT-B-16_rev_v1.1.3</code>), you do <strong>NOT</strong> have to use the <code class="notranslate">--hf</code> flag to download the<br>
model files from the HF ğŸ˜Š repo <sup><a href="#user-content-fn-1-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-1-6-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> ğŸ”— (only for the <strong>model version update</strong>).</p>
</div>
<p>You should see a message about loading the model from the hub and then saving it locally on<br>
your machine ğŸ–¥ï¸.</p>
<p>Only after you have obtained the trained model files (takes less time âŒ› than installing dependencies),<br>
you can play with any commands provided <a href="#how-to-run-prediction--modes">below</a>.</p>
<p>After the model is downloaded, you should see a similar file structure:</p>
<details>
<summary>Initial project tree ğŸŒ³ files structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">/local/folder/for/this/project/atrium-page-classification
â”œâ”€â”€ models
    â””â”€â”€ &lt;base_code&gt;_rev_&lt;revision&gt; 
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â””â”€â”€ preprocessor_config.json
â”œâ”€â”€ model_checkpoints
    â”œâ”€â”€ model_&lt;categ_limit&gt;_&lt;base_code&gt;_&lt;lr&gt;_&lt;epoch&gt;e.pt
    â”œâ”€â”€ model_&lt;categ_limit&gt;_&lt;base_code&gt;_&lt;lr&gt;_&lt;epoch&gt;e_cp.pt
    â””â”€â”€ ...
â”œâ”€â”€ data_scripts
    â”œâ”€â”€ windows
        â”œâ”€â”€ move_single.bat
        â”œâ”€â”€ pdf2png.bat
        â””â”€â”€ sort.bat
    â””â”€â”€ unix
        â”œâ”€â”€ move_single.sh
        â”œâ”€â”€ pdf2png.sh
        â””â”€â”€ sort.sh
â”œâ”€â”€ result
    â”œâ”€â”€ plots
        â”œâ”€â”€ date-time_&lt;#samples&gt;_EVAL_TOP-&lt;top_N&gt;_&lt;base&gt;_&lt;revision&gt;.png
        â””â”€â”€ ...
    â””â”€â”€ tables
        â”œâ”€â”€ date-time_&lt;#samples&gt;_EVAL_TOP-&lt;top_N&gt;_&lt;base&gt;_&lt;revision&gt;.csv
        â”œâ”€â”€ date-time_&lt;#samples&gt;_result_modelL_&lt;base&gt;_&lt;revision&gt;_TOP-&lt;top_N&gt;.csv
        â”œâ”€â”€ date-time_&lt;#samples&gt;_EVAL_&lt;base&gt;_&lt;revision&gt;_RAW.csv
        â””â”€â”€ ...
    â””â”€â”€ stats
        â”œâ”€â”€ model_accuracies.csv
        â”œâ”€â”€ model_accuracies_zero.csv
        â”œâ”€â”€ model_accuracies_plot.png
        â”œâ”€â”€ model_accuracies_zero_plot.png
        â”œâ”€â”€ date-time_model_&lt;rev&gt;_&lt;max_categ&gt;c_&lt;seed&gt;r_DATASETS.txt
        â””â”€â”€ ...
â”œâ”€â”€ category_samples
    â”œâ”€â”€ DRAW
        â”œâ”€â”€ CTX193200994-24.png
        â””â”€â”€ ...
    â”œâ”€â”€ DRAW_L
    â””â”€â”€ ...
â”œâ”€â”€ category_descriptions
    â”œâ”€â”€ page_categories_init.csv
    â”œâ”€â”€ TOTAL_page_categories.csv
    â””â”€â”€ ...
â”œâ”€â”€ run.py
â”œâ”€â”€ classifier.py
â”œâ”€â”€ minor_classes.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.txt
â”œâ”€â”€ README.md
â””â”€â”€ ...
</code></pre>
</details>
<p>Some of the folders may be missing, like mentioned <a href="#for-developers-">later</a> <code class="notranslate">model_output</code> which is automatically created<br>
only after launching the model.</p>
<hr>
<h2>How to run prediction ğŸª„ modes</h2>
<p>There are two main ways to run the program:</p>
<ul>
<li><strong>Single PNG file classification</strong> ğŸ“„</li>
<li><strong>Directory with PNG files classification</strong> ğŸ“</li>
</ul>
<p>To begin with, open <a href="config.txt">config.txt</a> âš™ and change folder path in the <code class="notranslate">[INPUT]</code> section, then<br>
optionally change <code class="notranslate">top_N</code> and <code class="notranslate">batch</code> in the <code class="notranslate">[SETUP]</code> section.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>ï¸ <strong>Top-3</strong> is enough to cover most of the images, setting <strong>Top-5</strong> will help with a small number<br>
of difficult to classify samples.</p>
</div>
<p>The <code class="notranslate">batch</code> variable value depends on your machine ğŸ–¥ï¸ memory size</p>
<details>
<summary>Rough estimations of memory usage per batch size ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th><strong>Batch size</strong></th>
<th><strong>CPU / GPU memory usage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>2 Gb</td>
</tr>
<tr>
<td>8</td>
<td>3 Gb</td>
</tr>
<tr>
<td>16</td>
<td>5 Gb</td>
</tr>
<tr>
<td>32</td>
<td>9 Gb</td>
</tr>
<tr>
<td>64</td>
<td>17 Gb</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p>It is safe to use batch size below <strong>12</strong> for a regular office desktop computer, and lower it to <strong>4</strong> if it's an old device.<br>
For training on a High Performance Computing cluster, you may use values above <strong>20</strong> for<br>
the <code class="notranslate">batch</code> variable in the <code class="notranslate">[SETUP]</code> section.</p>
<div class="markdown-alert markdown-alert-caution"><p class="markdown-alert-title"><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>Do <strong>NOT</strong> try to change <strong>base_model</strong> and other section contents unless you know what you are doing</p>
</div>
<details>
<summary>Rough estimations of disk space needed for trained model in relation to the base model ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th><strong>Version</strong></th>
<th><strong>Disk space</strong></th>
<th><strong>Parameters (Millions)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="notranslate">openai/clip-vit-base-patch16</code></td>
<td>992 Mb</td>
<td>149.62 M</td>
</tr>
<tr>
<td><code class="notranslate">openai/clip-vit-base-patch32</code></td>
<td>1008 Mb</td>
<td>151.28 M</td>
</tr>
<tr>
<td><code class="notranslate">openai/clip-vit-large-patch14</code></td>
<td>1.5 Gb</td>
<td>427.62 M</td>
</tr>
<tr>
<td><code class="notranslate">openai/clip-vit-large-patch14-336</code></td>
<td>1.5 Gb</td>
<td>427.94 M</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p>Make sure the virtual environment with all the installed libraries is activated, you are in the project<br>
directory with Python files and only then proceed.</p>
<details>
<summary>How to ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">cd /local/folder/for/this/project/
source &lt;your_venv_dir&gt;/bin/activate
cd atrium-page-classification
</code></pre>
</details>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>All the listed below commands for Python scripts running are adapted for <strong>Unix</strong> consoles, while<br>
<strong>Windows</strong> users must use <code class="notranslate">python</code> instead of <code class="notranslate">python3</code> syntax</p>
</div>
<h3>Page processing ğŸ“„</h3>
<p>The following prediction should be run using the <code class="notranslate">-f</code> or <code class="notranslate">--file</code> flag with the path argument. Optionally,<br>
you can use the <code class="notranslate">-tn</code> or <code class="notranslate">--topn</code> flag with the number of guesses you want to get, and also the <code class="notranslate">-m</code> or<br>
<code class="notranslate">--model</code> flag with the path to the model folder argument.</p>
<details>
<summary>How to ğŸ‘€</summary>
<p>Run the program from its starting point <a href="run.py">run.py</a> ğŸ“ with optional flags:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py -tn 3 -f '/full/path/to/file.png' --model_path '/full/path/to/model/folder' -m '&lt;base_code&gt;'
</code></pre>
<p>for exactly TOP-3 guesses with a console output.</p>
<p><strong>OR</strong> if you are sure about default variables set in the <a href="config.txt">config.txt</a> âš™:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py -f '/full/path/to/file.png'
</code></pre>
<p>to run a single PNG file classification - the output will be in the console.</p>
</details>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>Console output and all result tables contain <strong>normalized</strong> scores for the highest N class ğŸª§  scores</p>
</div>
<h3>Directory processing ğŸ“</h3>
<p>The following prediction type does <strong>NOT</strong> require explicit directory path setting with the <code class="notranslate">-d</code> or <code class="notranslate">--directory</code>,<br>
since its default value is set in the <a href="config.txt">config.txt</a> âš™ file and awakens when the <code class="notranslate">--dir</code> flag<br>
is used. The same flags for the number of guesses and the model folder path as for the single-page<br>
processing can be used. In addition, a directory-specific flag <code class="notranslate">--raw</code> is available.</p>
<div class="markdown-alert markdown-alert-caution"><p class="markdown-alert-title"><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>You must either explicitly set the <code class="notranslate">-d</code> flag's argument or use the <code class="notranslate">--dir</code> flag (calling for the preset in<br>
<code class="notranslate">[INPUT]</code> section default value of the input directory) to process PNG files on the directory<br>
level, otherwise, nothing will happen</p>
</div>
<p>Worth mentioning that the <strong>directory ğŸ“ level processing is performed in batches</strong>, therefore you should refer to<br>
the hardware's memory capacity requirements for different batch sizes tabulated <a href="#how-to-run-prediction--modes">above</a>.</p>
<details>
<summary>How to ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">python3 run.py -tn 3 -d '/full/path/to/directory' --model_path '/full/path/to/model/folder' -m '&lt;base_code&gt;'
</code></pre>
<p>for exactly TOP-3 guesses in tabular format from all images found in the given directory.</p>
<p><strong>OR</strong> if you are really sure about default variables set in the <a href="config.txt">config.txt</a> âš™:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --dir 

python3 run.py -rev v1.2 -m ViT-B/32 --dir
</code></pre>
</details>
<p>The classification results of PNG pages collected from the directory will be saved ğŸ’¾ to related <a href="result">results</a> ğŸ“<br>
folders defined in <code class="notranslate">[OUTPUT]</code> section of <a href="config.txt">config.txt</a> âš™ file.</p>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>To additionally get raw class ğŸª§ probabilities from the model along with the TOP-N results, use<br>
<code class="notranslate">--raw</code> flag when processing the directory (<strong>NOT</strong> available for single file processing)</p>
</div>
<p>Naturally, processing of the large amount of PNG pages takes time âŒ› and progress of this process<br>
is recorded in the console via messages like <code class="notranslate">Processed &lt;BÃ—N&gt; images</code> where <code class="notranslate">B</code><br>
is batch size set in the <code class="notranslate">[SETUP]</code> section of the <a href="config.txt">config.txt</a> âš™ file,<br>
and <code class="notranslate">N</code> is an iteration of the current dataloader processing loop.</p>
<p>Only after all images from the input directory are processed, the output table is<br>
saved ğŸ’¾ in the <code class="notranslate">result/tables</code> folder.</p>
<hr>
<h2>Results ğŸ“Š</h2>
<p>There are accuracy performance measurements and plots of confusion matrices for the evaluation<br>
dataset (10% of the provided in <code class="notranslate">[TRAIN]</code>'s folder data). Both graphic plots and tables with<br>
results can be found in the <a href="result">result</a> ğŸ“ folder.</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th align="left">Version</th>
<th align="left">Base Model + category set</th>
<th align="right">Accuracy (%)</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong>v1.1.3.1</strong></td>
<td align="left"><strong>ViT-B/16 init</strong></td>
<td align="right"><strong>99.1</strong></td>
<td>Very good</td>
</tr>
<tr>
<td align="left">v1.1.3.2</td>
<td align="left">ViT-B/16 details</td>
<td align="right">99.08</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>v1.1.3.3</strong></td>
<td align="left"><strong>ViT-B/16 extra</strong></td>
<td align="right"><strong>99.12</strong></td>
<td>2nd Best</td>
</tr>
<tr>
<td align="left"><strong>v1.1.3.4</strong></td>
<td align="left"><strong>ViT-B/16 gemini</strong></td>
<td align="right"><strong>99.1</strong></td>
<td>Very good</td>
</tr>
<tr>
<td align="left">v1.1.3.5</td>
<td align="left">ViT-B/16 gpt</td>
<td align="right">98.95</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>v1.1.3.6</strong></td>
<td align="left"><strong>ViT-B/16 large</strong></td>
<td align="right"><strong>99.1</strong></td>
<td>Very good</td>
</tr>
<tr>
<td align="left"><strong>v1.1.3.7</strong></td>
<td align="left"><strong>ViT-B/16 mid</strong></td>
<td align="right"><strong>99.14</strong></td>
<td>Best</td>
</tr>
<tr>
<td align="left">v1.1.3.8</td>
<td align="left">ViT-B/16 min</td>
<td align="right">98.86</td>
<td></td>
</tr>
<tr>
<td align="left">v1.1.3.9</td>
<td align="left">ViT-B/16 short</td>
<td align="right">99.06</td>
<td></td>
</tr>
<tr>
<td align="left">v1.1.3</td>
<td align="left">ViT-B/16 average</td>
<td align="right">99.06</td>
<td></td>
</tr>
<tr>
<td align="left">v1.2.3.1</td>
<td align="left">ViT-B/32 init</td>
<td align="right">98.95</td>
<td></td>
</tr>
<tr>
<td align="left">v1.2.3.3</td>
<td align="left">ViT-B/32 extra</td>
<td align="right">98.92</td>
<td></td>
</tr>
<tr>
<td align="left">v1.2.3.4</td>
<td align="left">ViT-B/32 gemini</td>
<td align="right">98.94</td>
<td></td>
</tr>
<tr>
<td align="left">v1.2.3.6</td>
<td align="left">ViT-B/32 large</td>
<td align="right">98.97</td>
<td></td>
</tr>
<tr>
<td align="left">v1.2.3.7</td>
<td align="left">ViT-B/32 mid</td>
<td align="right">98.86</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>v1.2.3</strong></td>
<td align="left"><strong>ViT-B/32 average</strong></td>
<td align="right"><strong>98.99</strong></td>
<td>Larger &amp; good</td>
</tr>
<tr>
<td align="left"><strong>v2.2.3.1</strong></td>
<td align="left"><strong>ViT-L/14-336px init</strong></td>
<td align="right"><strong>98.86</strong></td>
<td>Large &amp; OK</td>
</tr>
<tr>
<td align="left">v2.2.3.3</td>
<td align="left">ViT-L/14-336px extra</td>
<td align="right">98.59</td>
<td></td>
</tr>
<tr>
<td align="left">v2.2.3.4</td>
<td align="left">ViT-L/14-336px gemini</td>
<td align="right">98.97</td>
<td></td>
</tr>
<tr>
<td align="left">v2.2.3.6</td>
<td align="left">ViT-L/14-336px large</td>
<td align="right">98.68</td>
<td></td>
</tr>
<tr>
<td align="left">v2.2.3.7</td>
<td align="left">ViT-L/14-336px mid</td>
<td align="right">98.81</td>
<td></td>
</tr>
<tr>
<td align="left">v2.2.3</td>
<td align="left">ViT-L/14-336px average</td>
<td align="right">98.72</td>
<td></td>
</tr>
<tr>
<td align="left">v2.1.3.1</td>
<td align="left">ViT-L/14 init</td>
<td align="right">98.97</td>
<td></td>
</tr>
<tr>
<td align="left">v2.1.3.3</td>
<td align="left">ViT-L/14 extra</td>
<td align="right">98.83</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>v2.1.3.4</strong></td>
<td align="left"><strong>ViT-L/14 gemini</strong></td>
<td align="right"><strong>98.86</strong></td>
<td>Large &amp; OK</td>
</tr>
<tr>
<td align="left">v2.1.3.6</td>
<td align="left">ViT-L/14 large</td>
<td align="right">98.92</td>
<td></td>
</tr>
<tr>
<td align="left">v2.1.3.7</td>
<td align="left">ViT-L/14 mid</td>
<td align="right">98.9</td>
<td></td>
</tr>
<tr>
<td align="left">v2.1.3</td>
<td align="left">ViT-L/14 average</td>
<td align="right">98.81</td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p><code class="notranslate">v1.1.3.1</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>99.1%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20251022-0335_5449_EVAL_TOP-1_ViT-B16_v1131.png"><img src="result%2Fplots%2F20251022-0335_5449_EVAL_TOP-1_ViT-B16_v1131.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v1.1.3.3</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>99.12%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20251022-1435_5449_EVAL_TOP-1_ViT-B16_v1133.png"><img src="result%2Fplots%2F20251022-1435_5449_EVAL_TOP-1_ViT-B16_v1133.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v1.1.3.4</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>99.1%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20251022-0411_5449_EVAL_TOP-1_ViT-B16_v1134.png"><img src="result%2Fplots%2F20251022-0411_5449_EVAL_TOP-1_ViT-B16_v1134.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v1.1.3.6</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>99.1%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20251022-0435_5449_EVAL_TOP-1_ViT-B16_v1136.png"><img src="result%2Fplots%2F20251022-0435_5449_EVAL_TOP-1_ViT-B16_v1136.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v1.1.3.7</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>99.14%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20251022-1656_5449_EVAL_TOP-1_ViT-B16_v1137.png"><img src="result%2Fplots%2F20251022-1656_5449_EVAL_TOP-1_ViT-B16_v1137.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v1.2.3</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>98.99%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20251022-1718_5449_EVAL_TOP-1_ViT-B32_v123.png"><img src="result%2Fplots%2F20251022-1718_5449_EVAL_TOP-1_ViT-B32_v123.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v2.1.3.1</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>98.97%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20251022-1733_5449_EVAL_TOP-1_ViT-L14_v2131.png"><img src="result%2Fplots%2F20251022-1733_5449_EVAL_TOP-1_ViT-L14_v2131.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<p><code class="notranslate">v2.2.3.4</code> Evaluation set's accuracy (<strong>Top-1</strong>):  <strong>98.97%</strong> ğŸ†</p>
<details>
<summary>Confusion matrix ğŸ“Š TOP-1 ğŸ‘€</summary>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fplots%2F20251022-1457_5449_EVAL_TOP-1_ViT-L14-336px_v2234.png"><img src="result%2Fplots%2F20251022-1457_5449_EVAL_TOP-1_ViT-L14-336px_v2234.png" alt="TOP-1 confusion matrix" style="max-width: 100%;"></a></p>
</details>
<blockquote>
<p><strong>Confusion matrices</strong> provided above show the diagonal of matching gold and predicted categories ğŸª§<br>
while their off-diagonal elements show inter-class errors. By those graphs you can judge<br>
<strong>what type of mistakes to expect</strong> from your model.</p>
</blockquote>
<p>By running tests on the evaluation dataset after training you can generate the following output files:</p>
<ul>
<li><strong>EVAL_table_Nn_Cc__date-time.csv</strong> - (by default) results of the evaluation dataset with TOP-N guesses</li>
<li><strong>conf_mat_Nn_Cc__date-time.png</strong> - (by default) confusion matrix plot for the evaluation dataset also with TOP-N guesses</li>
<li><strong>date-time__RAW.csv</strong> - (by flag <code class="notranslate">--raw</code>) raw probabilities for all classes of the processed directory</li>
<li><strong>result_date-time__Nn_Cc.csv</strong> - (by default) results of the processed directory with TOP-N guesses</li>
</ul>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>Generated tables will be sorted by <strong>FILE</strong> and <strong>PAGE</strong> number columns in ascending order.</p>
</div>
<p>Additionally, results of prediction inference run on the directory level without checked results are included.</p>
<h3>Result tables and their columns ğŸ“ğŸ“‹</h3>
<details>
<summary>General result tables ğŸ‘€</summary>
<p>Demo files  <code class="notranslate">v1.1.3.1</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20251022-0335_5449_EVAL_TOP-1_ViT-B16_v1131.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20251022-0059_5449_EVAL_TOP-3_ViT-B16_v1131.csv">model_TOP-53EVAL.csv</a> ğŸ“</p>
</li>
<li></li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20251023-1412_114_result_ViT-B16_v1.1.3.1_TOP-1.csv">model_TOP-1.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v1.1.3.3</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20251022-1435_5449_EVAL_TOP-1_ViT-B16_v1133.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20251022-1436_5449_EVAL_TOP-3_ViT-B16_v1133.csv">model_TOP-53EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20251022-1223_114_result_ViT-B16_v1.1.3.3_TOP-1.csv">model_TOP-1.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v1.1.3.4</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20251022-0411_5449_EVAL_TOP-1_ViT-B16_v1134.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20251022-0140_5449_EVAL_TOP-3_ViT-B16_v1134.csv">model_TOP-53EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20251023-1413_114_result_ViT-B16_v1.1.3.4_TOP-1.csv">model_TOP-1.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v1.1.3.6</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20251022-0435_5449_EVAL_TOP-1_ViT-B16_v1136.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20251022-0207_5449_EVAL_TOP-3_ViT-B16_v1136.csv">model_TOP-53EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20251023-1413_114_result_ViT-B16_v1.1.3.6_TOP-1.csv">model_TOP-1.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v1.1.3.7</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20251022-1656_5449_EVAL_TOP-1_ViT-B16_v1137.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20251022-1524_5449_EVAL_TOP-3_ViT-B16_v1137.csv">model_TOP-53EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20251022-1224_114_result_ViT-B16_v1.1.3.7_TOP-1.csv">model_TOP-1.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v1.2.3</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20251022-1718_5449_EVAL_TOP-1_ViT-B32_v123.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20251022-1537_5449_EVAL_TOP-3_ViT-B32_v123.csv">model_TOP-53EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20251022-1226_114_result_ViT-B32_v1.2.3_TOP-1.csv">model_TOP-1.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v2.1.3.1</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20251022-1733_5449_EVAL_TOP-1_ViT-L14_v2131.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li></li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20251022-1551_5449_EVAL_TOP-3_ViT-L14_v2131.csv">model_TOP-53EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20251022-1226_114_result_ViT-L14_v2.1.3.1_TOP-1.csv">model_TOP-1.csv</a>ğŸ“</p>
</li>
</ul>
<p>Demo files  <code class="notranslate">v2.2.3.4</code>:</p>
<ul>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-1): <a href="result%2Ftables%2F20251022-1457_5449_EVAL_TOP-1_ViT-L14-336px_v2234.csv">model_TOP-1_EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p>Manually âœï¸ <strong>checked</strong> evaluation dataset (TOP-3): <a href="result%2Ftables%2F20251022-1452_5449_EVAL_TOP-3_ViT-L14-336px_v2234.csv">model_TOP-53EVAL.csv</a> ğŸ“</p>
</li>
<li>
<p><strong>Unchecked with TRUE</strong> values (small): <a href="result%2Ftables%2F20251022-1230_114_result_ViT-L14-336px_v2.2.3.4_TOP-1.csv">model_TOP-1.csv</a>ğŸ“</p>
</li>
</ul>
<p>With the following <strong>columns</strong> ğŸ“‹:</p>
<ul>
<li><strong>FILE</strong> - name of the file</li>
<li><strong>PAGE</strong> - number of the page</li>
<li><strong>CLASS-N</strong> - label of the category ğŸª§, guess TOP-N</li>
<li><strong>SCORE-N</strong> - score of the category ğŸª§, guess TOP-N</li>
</ul>
<p>and optionally</p>
<ul>
<li><strong>TRUE</strong> - actual label of the category ğŸª§</li>
</ul>
</details>
<details>
<summary>Raw result tables ğŸ‘€</summary>
<p>Demo files <code class="notranslate">v1.1</code>:</p>
<ul>
<li><strong>Unchecked with TRUE</strong> values (small) <strong>RAW</strong>: <a href="result%2Ftables%2F20250701-1816_ViT-B16_RAW.csv">model_RAW.csv</a> ğŸ“</li>
</ul>
<p>Demo files <code class="notranslate">v1.2</code>:</p>
<ul>
<li><strong>Unchecked with TRUE</strong> values (small) <strong>RAW</strong>: <a href="result%2Ftables%2F20250701-2216_ViT-B32_RAW.csv">model_RAW.csv</a> ğŸ“</li>
</ul>
<p>Demo files <code class="notranslate">v2.1</code>:</p>
<ul>
<li><strong>Unchecked with TRUE</strong> values (small) <strong>RAW</strong>: <a href="result%2Ftables%2F20250701-1743_ViT-L14_RAW.csv">model_RAW.csv</a> ğŸ“</li>
</ul>
<p>Demo files <code class="notranslate">v2.2</code>:</p>
<ul>
<li><strong>Unchecked with TRUE</strong> values (small) <strong>RAW</strong>: <a href="result%2Ftables%2F20250701-2218_ViT-L14@336px_RAW.csv">model_RAW.csv</a> ğŸ“</li>
</ul>
<p>With the following <strong>columns</strong> ğŸ“‹:</p>
<ul>
<li><strong>FILE</strong> - name of the file</li>
<li><strong>PAGE</strong> - number of the page</li>
<li><strong>&lt;CATEGORY_LABEL&gt;</strong> - separate columns for each of the defined classes ğŸª§</li>
</ul>
</details>
<p>The reason to use the <code class="notranslate">--raw</code> flag is the possible convenience of results review,<br>
since the rows will be basically sorted by categories, and most ambiguous ones will<br>
have more small probabilities instead of zeros than the most obvious (for the model)<br>
categories ğŸª§.</p>
<p>Plus the small  <strong>Unchecked with TRUE</strong> values combination of the best 5 models: <a href="result%2Ftables%2F20251023-1412_BEST_5_models_TOP-1.csv">models_TOP-1.csv</a>ğŸ“</p>
<hr>
<h2>Data preparation ğŸ“¦</h2>
<p>You can use this section as a guide for creating your own dataset of pages, which will be suitable for<br>
further model processing.</p>
<p>There are useful multiplatform scripts in the <a href="data_scripts">data_scripts</a> ğŸ“ folder for the whole process of data preparation.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>The <code class="notranslate">.sh</code> scripts are adapted for <strong>Unix</strong> OS and <code class="notranslate">.bat</code> scripts are adapted for <strong>Windows</strong> OS, yet<br>
their functionality remains the same</p>
</div>
<p>On <strong>Windows</strong> you must also install the following software before converting PDF documents to PNG images:</p>
<ul>
<li>ImageMagick <sup><a href="#user-content-fn-5-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-5-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup> ğŸ”— - download and install the latest version</li>
<li>Ghostscript <sup><a href="#user-content-fn-6-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-6-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup> ğŸ”— - download and install the latest version (32 or 64-bit) by AGPL</li>
</ul>
<h3>PDF to PNG ğŸ“š</h3>
<p>The source set of PDF documents must be converted to page-specific PNG images before processing. The following steps<br>
describe the procedure of converting PDF documents to PNG images suitable for training, evaluation, or prediction inference.</p>
<p>Firstly, copy the PDF-to-PNG converter script to the directory with PDF documents.</p>
<details>
<summary>How to ğŸ‘€</summary>
<p><strong>Windows</strong>:</p>
<pre class="notranslate"><code class="notranslate">move \local\folder\for\this\project\data_scripts\pdf2png.bat \full\path\to\your\folder\with\pdf\files
</code></pre>
<p><strong>Unix</strong>:</p>
<pre class="notranslate"><code class="notranslate">cp /local/folder/for/this/project/data_scripts/pdf2png.sh /full/path/to/your/folder/with/pdf/files
</code></pre>
</details>
<p>Now check the content and comments in <a href="data_scripts%2Funix%2Fpdf2png.sh">pdf2png.sh</a> ğŸ“ or <a href="data_scripts%2Fwindows%2Fpdf2png.bat">pdf2png.bat</a> ğŸ“<br>
script, and run it.</p>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>You can optionally comment out the <strong>removal of processed PDF files</strong> from the script, yet it's <strong>NOT</strong><br>
recommended in case you are going to launch the program several times from the same location.</p>
</div>
<details>
<summary>How to ğŸ‘€</summary>
<p><strong>Windows</strong>:</p>
<pre class="notranslate"><code class="notranslate">cd \full\path\to\your\folder\with\pdf\files
pdf2png.bat
</code></pre>
<p><strong>Unix</strong>:</p>
<pre class="notranslate"><code class="notranslate">cd /full/path/to/your/folder/with/pdf/files
pdf2png.sh
</code></pre>
</details>
<p>After the program is done, you will have a directory full of document-specific subdirectories<br>
containing page-specific images with a similar structure:</p>
<details>
<summary>Unix folder tree ğŸŒ³ structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">/full/path/to/your/folder/with/pdf/files
â”œâ”€â”€ PdfFile1Name
    â”œâ”€â”€ PdfFile1Name-001.png
    â”œâ”€â”€ PdfFile1Name-002.png
    â””â”€â”€ ...
â”œâ”€â”€ PdfFile2Name
    â”œâ”€â”€ PdfFile2Name-01.png
    â”œâ”€â”€ PDFFile2Name-02.png
    â””â”€â”€ ...
â”œâ”€â”€ PdfFile3Name
    â””â”€â”€ PdfFile3Name-1.png 
â”œâ”€â”€ PdfFile4Name
â””â”€â”€ ...
</code></pre>
</details>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>The page numbers are padded with zeros (on the left) to match the length of the last page number in each PDF file,<br>
this is done automatically by the pdftoppm command used on <strong>Unix</strong>. While ImageMagick's <sup><a href="#user-content-fn-5-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-5-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup> ğŸ”— convert command used<br>
on <strong>Windows</strong> does <strong>NOT</strong> pad the page numbers.</p>
</div>
<details>
<summary>Windows folder tree ğŸŒ³ structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">\full\path\to\your\folder\with\pdf\files
â”œâ”€â”€ PdfFile1Name
    â”œâ”€â”€ PdfFile1Name-1.png
    â”œâ”€â”€ PdfFile1Name-2.png
    â””â”€â”€ ...
â”œâ”€â”€ PdfFile2Name
    â”œâ”€â”€ PdfFile2Name-1.png
    â”œâ”€â”€ PDFFile2Name-2.png
    â””â”€â”€ ...
â”œâ”€â”€ PdfFile3Name
    â””â”€â”€ PdfFile3Name-1.png 
â”œâ”€â”€ PdfFile4Name
â””â”€â”€ ...
</code></pre>
</details>
<p>Optionally you can use the <a href="data_scripts%2Funix%2Fmove_single.sh">move_single.sh</a> ğŸ“ or <a href="data_scripts%2Fwindows%2Fmove_single.bat">move_single.bat</a> ğŸ“ script to move<br>
all PNG files from directories with a single PNG file inside to the common directory of one-pagers.</p>
<p>By default, the scripts assume that the <code class="notranslate">onepagers</code> is the back-off directory for PDF document names without a<br>
corresponding separate directory of PNG pages found in the PDF files directory (already converted to<br>
subdirectories of pages).</p>
<details>
<summary>How to ğŸ‘€</summary>
<p><strong>Windows</strong>:</p>
<pre class="notranslate"><code class="notranslate">move \local\folder\for\this\project\atrium-page-classification\data_scripts\move_single.bat \full\path\to\your\folder\with\pdf\files
cd \full\path\to\your\folder\with\pdf\files
move_single.bat
</code></pre>
<p><strong>Unix</strong>:</p>
<pre class="notranslate"><code class="notranslate">cp /local/folder/for/this//project/atrium-page-classification/data_scripts/move_single.sh /full/path/to/your/folder/with/pdf/files
cd /full/path/to/your/folder/with/pdf/files 
move_single.sh 
</code></pre>
</details>
<p>The reason for such movement is simply convenience in the following annotation process <a href="#png-pages-annotation-">below</a>.<br>
These changes are cared for in the next <a href="data_scripts%2Funix%2Fsort.sh">sort.sh</a> ğŸ“ and <a href="data_scripts%2Fwindows%2Fsort.bat">sort.bat</a> ğŸ“ scripts as well.</p>
<h3>PNG pages annotation ğŸ”</h3>
<p>The generated PNG images of document pages are used to form the annotated gold data.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>It takes a lot of time âŒ› to collect at least several hundred examples per category.</p>
</div>
<p>Prepare a CSV table with exactly 3 columns:</p>
<ul>
<li><strong>FILE</strong> - name of the PDF document which was the source of this page</li>
<li><strong>PAGE</strong> - number of the page (<strong>NOT</strong> padded with 0s)</li>
<li><strong>CLASS</strong> - label of the category ğŸª§</li>
</ul>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>Prepare equal-in-size categories ğŸª§ if possible, so that the model will not be biased towards the over-represented labels ğŸª§</p>
</div>
<p>For <strong>Windows</strong> users, it's <strong>NOT</strong> recommended to use MS Excel for writing CSV tables, the free<br>
alternative may be Apache's OpenOffice <sup><a href="#user-content-fn-9-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-9-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup> ğŸ”—. As for <strong>Unix</strong> users, the default LibreCalc should be enough to<br>
correctly write a comma-separated CSV table.</p>
<details>
<summary>Table in .csv format example ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">FILE,PAGE,CLASS
PdfFile1Name,1,Label1
PdfFile2Name,9,Label1
PdfFile1Name,11,Label3
...
</code></pre>
</details>
<h3>PNG pages sorting for training ğŸ“¬</h3>
<p>Cluster the annotated data into separate folders using the <a href="data_scripts%2Funix%2Fsort.sh">sort.sh</a> ğŸ“ or <a href="data_scripts%2Fwindows%2Fsort.bat">sort.bat</a> ğŸ“<br>
script to copy data from the source folder to the training folder where each category ğŸª§ has its own subdirectory.<br>
This division of PNG images will be used as gold data in training and evaluation.</p>
<div class="markdown-alert markdown-alert-warning"><p class="markdown-alert-title"><svg class="octicon octicon-alert mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Warning</p><p>It does <strong>NOT</strong> matter from which directory you launch the sorting script, but you must check the top of the script for<br>
(<strong>1</strong>) the path to the previously described <strong>CSV table with annotations</strong>, (<strong>2</strong>) the path to the previously described<br>
directory containing <strong>document-specific subdirectories of page-specific PNG pages</strong>, and (<strong>3</strong>) the path to the directory<br>
where you want to store the <strong>training data of label-specific directories with annotated page images</strong>.</p>
</div>
<details>
<summary>How to ğŸ‘€</summary>
<p><strong>Windows</strong>:</p>
<pre class="notranslate"><code class="notranslate">sort.bat
</code></pre>
<p><strong>Unix</strong>:</p>
<pre class="notranslate"><code class="notranslate">sort.sh
</code></pre>
</details>
<p>After the program is done, you will have a directory full of label-specific subdirectories<br>
containing document-specific pages with a similar structure:</p>
<details>
<summary>Unix folder tree ğŸŒ³ structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">/full/path/to/your/folder/with/train/pages
â”œâ”€â”€ Label1
    â”œâ”€â”€ PdfFileAName-00N.png
    â”œâ”€â”€ PdfFileBName-0M.png
    â””â”€â”€ ...
â”œâ”€â”€ Label2
â”œâ”€â”€ Label3
â”œâ”€â”€ Label4
â””â”€â”€ ...
</code></pre>
</details>
<details>
<summary>Windows folder tree ğŸŒ³ structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">\full\path\to\your\folder\with\train\pages
â”œâ”€â”€ Label1
    â”œâ”€â”€ PdfFileAName-N.png
    â”œâ”€â”€ PdfFileBName-M.png
    â””â”€â”€ ...
â”œâ”€â”€ Label2
â”œâ”€â”€ Label3
â”œâ”€â”€ Label4
â””â”€â”€ ...
</code></pre>
</details>
<p>The sorting script can help you in moderating mislabeled samples before the training. Accurate data annotation<br>
directly affects the model performance.</p>
<p>Before running the training, make sure to check the <a href="config.txt">config.txt</a> âš™ï¸ file for the <code class="notranslate">[TRAIN]</code> section variables, where you should<br>
set a path to the data folder. Make sure label directory names do <strong>NOT</strong> contain special characters like spaces, tabs or paragraph splits.</p>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>In the <a href="config.txt">config.txt</a> âš™ï¸ file tweak the parameter of <code class="notranslate">max_categ</code><br>
for a maximum number of samples per category ğŸª§, in case you have <strong>over-represented labels</strong> significantly dominating in size.<br>
Set <code class="notranslate">max_categ</code> higher than the number of samples in the largest category ğŸª§ to use <strong>all</strong> data samples. Similarly,<br>
<code class="notranslate">max_categ_e</code> parameter sets the maximum number of samples per category ğŸª§ for the evaluation dataset, and should be<br>
increased to very large numbers if you want to cover all samples from all categories ğŸª§.</p>
</div>
<p>From this point, you can start model training or evaluation process.</p>
<hr>
<h2>For developers ğŸª›</h2>
<p>You can use this project code as a base for your own image classification tasks. The detailed guide on<br>
the key phases of the whole process (settings, training, evaluation) is provided here.</p>
<details>
<summary>Project files description ğŸ“‹ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>File Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="notranslate">classifier.py</code></td>
<td>Model-specific classes and related functions including predefined values for training arguments</td>
</tr>
<tr>
<td><code class="notranslate">minor_classes.py</code></td>
<td>Adjacent functions and support classes</td>
</tr>
<tr>
<td><code class="notranslate">utils.py</code></td>
<td>Task-related algorithms</td>
</tr>
<tr>
<td><code class="notranslate">run.py</code></td>
<td>Starting point of the program with its main function - can be edited for flags and function argument extensions</td>
</tr>
<tr>
<td><code class="notranslate">config.txt</code></td>
<td>Changeable variables for the program - should be edited</td>
</tr>
<tr>
<td><code class="notranslate">job_run.sh</code></td>
<td>Running on a server node script</td>
</tr>
<tr>
<td><code class="notranslate">dataset_timeline.py</code></td>
<td>Creates a plot of categories distribution over time based on filenames</td>
</tr>
<tr>
<td><code class="notranslate">img2jpeg_v3.py</code></td>
<td>Transforms any images into jpeg format</td>
</tr>
<tr>
<td><code class="notranslate">logs_stats.py</code></td>
<td>Creates a table of stats for each tensorboard directory with event logs</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p>Most of the changeable variables are in the <a href="config.txt">config.txt</a> âš™ file, specifically,<br>
in the <code class="notranslate">[TRAIN]</code>, <code class="notranslate">[HF]</code>, and <code class="notranslate">[SETUP]</code> sections.</p>
<p>In the dev sections of the configuration âš™ file, you will find many boolean variables that can be changed from the default <code class="notranslate">False</code><br>
state to <code class="notranslate">True</code>, yet it's recommended to awaken those variables solely through the specific<br>
<strong>command line flags implemented for each of these boolean variables</strong>.</p>
<p>For more detailed training process adjustments refer to the related functions in <a href="classifier.py">classifier.py</a> ğŸ“<br>
file, where you will find some predefined values not used in the <a href="run.py">run.py</a> ğŸ“ file.</p>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>For both training and evaluation, you must make sure that the training pages directory is set right in the<br>
<a href="config.txt">config.txt</a> âš™ and it contains category ğŸª§ subdirectories with images inside.<br>
Names of the category ğŸª§ subdirectories are sorted in the alphabetic order and become actual<br>
label names and replace the default categories ğŸª§ list</p>
</div>
<p>Device ğŸ–¥ï¸ requirements for training / evaluation:</p>
<ul>
<li><strong>CPU</strong> of some kind and memory size</li>
<li><strong>GPU</strong> (for real CUDA <sup><a href="#user-content-fn-10-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-10-3-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup> support - better one of Nvidia's cards)</li>
</ul>
<p>Worth mentioning that the efficient training is possible only with a CUDA-compatible GPU card.</p>
<details>
<summary>Rough estimations of memory usage ğŸ‘€</summary>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th><strong>Batch size</strong></th>
<th><strong>CPU / GPU memory usage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>2 Gb</td>
</tr>
<tr>
<td>8</td>
<td>3 Gb</td>
</tr>
<tr>
<td>16</td>
<td>5 Gb</td>
</tr>
<tr>
<td>32</td>
<td>9 Gb</td>
</tr>
<tr>
<td>64</td>
<td>17 Gb</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p>For test launches on the <strong>CPU-only device ğŸ–¥ï¸</strong> you should set <strong>batch size to lower than 4</strong>, and even in this<br>
case, <strong>above-average CPU memory capacity</strong> is a must-have to avoid a total system crush.</p>
<h3>Training ğŸ’ª</h3>
<p>To train the model run:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --train
</code></pre>
<p>The training process has an automatic progress logging into console, and should take approximately 5-12h<br>
depending on your machine's ğŸ–¥ï¸ CPU / GPU memory size and prepared dataset size.</p>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>Run the training with <strong>default hyperparameters</strong> if you have at least ~10,000 and <strong>less than 50,000 page samples</strong><br>
of the very similar to the initial source data - meaning, no further changes are required for fine-tuning model<br>
for the same task on an expanded (or new) dataset of document pages, even number of categories ğŸª§ does<br>
<strong>NOT</strong> matter while it stays under <strong>20</strong></p>
</div>
<details>
<summary>Training hyperparameters ğŸ‘€</summary>
<ul>
<li>eval_strategy "epoch"</li>
<li>save_strategy "epoch"</li>
<li>learning_rate <strong>5e-5</strong></li>
<li>per_device_train_batch_size 8</li>
<li>per_device_eval_batch_size 8</li>
<li>num_train_epochs <strong>3</strong></li>
<li>warmup_ratio <strong>0.1</strong></li>
<li>logging_steps <strong>10</strong></li>
<li>load_best_model_at_end True</li>
<li>metric_for_best_model "accuracy"</li>
</ul>
</details>
<p>Above are the default hyperparameters or TrainingArguments <sup><a href="#user-content-fn-11-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-11-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup> used in the training process that can be partially<br>
(only <code class="notranslate">epoch</code> and <code class="notranslate">log_step</code>) changed in the <code class="notranslate">[TRAIN]</code> section, plus <code class="notranslate">batch</code> in the <code class="notranslate">[SETUP]</code>section,<br>
of the <a href="config.txt">config.txt</a> âš™ file. Importantly, <code class="notranslate">avg</code> - average configuration of all texts can be used.</p>
<div class="markdown-alert markdown-alert-important"><p class="markdown-alert-title"><svg class="octicon octicon-report mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p>CLIP models accept not only images but also text inputs, in our case its <a href="category_descriptions%2FTOTAL_page_categories.csv">descriptions.tsv</a> ğŸ“ file<br>
which summarizes the category ğŸª§ descriptions in the <a href="category_descriptions">category_descriptions</a> ğŸ“ folder. Optionally you can run the modesl<br>
with only a single table of category ğŸª§ descriptions (via <code class="notranslate">categories_file</code> variable), or use <code class="notranslate">--avg</code> flag to average all of the<br>
category ğŸª§ descriptions in the <code class="notranslate">description_folder</code> starting with the <code class="notranslate">categories_prefix</code> value.</p>
</div>
<p>In case your descriptions table contains <strong>more than 1 text per category ğŸª§</strong>, the <code class="notranslate">--avg</code> flag will be set to <code class="notranslate">True</code> automatically.</p>
<p><a href="result%2Fstats%2Fmodel_accuracy_plot.png">descriptions_comparison_graph.png</a> ğŸ“ is a graph containing separate and averaged results<br>
of all category ğŸª§ descriptions. Using averaged text embeddings of all label description seemed to be the most powerful way to<br>
classify our images based on the zero-shot evaluations.</p>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fstats%2Fmodel_accuracy_plot_zero.png"><img src="result%2Fstats%2Fmodel_accuracy_plot_zero.png" alt="description comparison graph_zero" style="max-width: 100%;"></a></p>
<p>As the following experiments showed, the averaging strategy is not the best. Moreover, the smallest model<br>
ViT-B/16 showed the best results after separately fine-tuning model on some category ğŸª§ sets.</p>
<p><a target="_blank" rel="noopener noreferrer" href="result%2Fstats%2Fmodel_accuracy_plot_all.png"><img src="result%2Fstats%2Fmodel_accuracy_plot_all.png" alt="description comparison graph" style="max-width: 100%;"></a></p>
<p>Check out all of the prepared category ğŸª§ descriptions in the <a href="category_descriptions">category_descriptions</a> ğŸ“ folder.<br>
Which supports versions mapping from 1 to 9 for the csv files starting with <code class="notranslate">page_categories_</code> prefix. The separate set<br>
starting with <code class="notranslate">TOTAL</code> is a mixture of all category descriptions, a set starting with <code class="notranslate">GENERAL</code> is a simplified category ğŸª§ set<br>
(only 4 classes), and a set starting with <code class="notranslate">EXPANDED</code> is an experimental more fine-grained in categories version of the category ğŸª§ descriptions.</p>
<blockquote>
<p>You are free to play with the <strong>learning rate</strong> right in the training function arguments called in the <a href="run.py">run.py</a> ğŸ“ file,<br>
yet <strong>warmup ratio and other hyperparameters</strong> are accessible only through the <a href="classifier.py">classifier.py</a> ğŸ“ file.</p>
</blockquote>
<p>Playing with training hyperparameters is<br>
recommended only if <strong>training ğŸ’ª loss</strong> (error rate) descends too slow to reach 0.001-0.001<br>
values by the end of the 3rd (last by default) epoch.</p>
<p>In the case <strong>evaluation ğŸ† loss</strong> starts to steadily going up after the previous descend, this means<br>
you have reached the limit of worthy epochs, and next time you should set <code class="notranslate">epochs</code> to the<br>
number of epoch that has successfully ended before you noticed the evaluation loss growth.</p>
<p>During training image transformations <sup><a href="#user-content-fn-12-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-12-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup> are applied sequentially with a 50% chance.</p>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>No rotation, reshaping, or flipping was applied to the images, mainly color manipulations were used. The<br>
reason behind this are pages containing specific form types, general text orientation on the pages, and the default<br>
reshape of the model input to the square 224x224 (or 336x336) resolution images.</p>
</div>
<details>
<summary>Image preprocessing steps ğŸ‘€</summary>
<ul>
<li>transforms.ColorJitter(<strong>brightness</strong> 0.5)</li>
<li>transforms.ColorJitter(<strong>contrast</strong> 0.5)</li>
<li>transforms.ColorJitter(<strong>saturation</strong> 0.5)</li>
<li>transforms.ColorJitter(<strong>hue</strong> 0.5)</li>
<li>transforms.Lambda(lambda img: ImageEnhance.<strong>Sharpness</strong>(img).enhance(random.uniform(0.5, 1.5)))</li>
<li>transforms.Lambda(lambda img: img.filter(ImageFilter.<strong>GaussianBlur</strong>(radius=random.uniform(0, 2))))</li>
</ul>
</details>
<p>More about selecting the image transformation and the available ones you can read in the PyTorch torchvision docs <sup><a href="#user-content-fn-12-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-12-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup>.</p>
<p>After training is complete the model will be saved ğŸ’¾ to its separate subdirectory in the <code class="notranslate">model</code> directory, by default,<br>
the <strong>naming of the model folder</strong> corresponds to the length of its training batch dataloader and the number of epochs -<br>
for example <code class="notranslate">model_&lt;S/B&gt;_E</code> where <code class="notranslate">E</code> is the number of epochs, <code class="notranslate">B</code> is the batch size, and <code class="notranslate">S</code> is the size of your<br>
<strong>training</strong> dataset (by defaults, 90% of the provided in <code class="notranslate">[TRAIN]</code>'s folder data).</p>
<details>
<summary>Full project tree ğŸŒ³ files structure ğŸ‘€</summary>
<pre class="notranslate"><code class="notranslate">/local/folder/for/this/project/atrium-page-classification
â”œâ”€â”€ models
    â”œâ”€â”€ &lt;base_code&gt;_rev_v&lt;HFrevision1&gt; 
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â””â”€â”€ preprocessor_config.json
    â”œâ”€â”€ &lt;base_code&gt;_rev_v&lt;HFrevision2&gt;
    â””â”€â”€ ...
â”œâ”€â”€ hf_hub_checkpoints
    â”œâ”€â”€ models--openai--clip-vit-base-patch16
        â”œâ”€â”€ blobs
        â”œâ”€â”€ snapshots
        â””â”€â”€ refs
    â””â”€â”€ .locs
        â””â”€â”€ models--openai--clip-vit-large-patch14
â”œâ”€â”€ model_checkpoints
    â”œâ”€â”€ model_&lt;categ_limit&gt;_&lt;base_code&gt;_&lt;lr&gt;.pt
    â”œâ”€â”€ model_&lt;categ_limit&gt;_&lt;base_code&gt;_&lt;lr&gt;_cp.pt
    â””â”€â”€ ...
â”œâ”€â”€ data_scripts
    â”œâ”€â”€ windows
    â””â”€â”€ unix
â”œâ”€â”€ result
    â”œâ”€â”€ plots
    â””â”€â”€ tables
â”œâ”€â”€ category_samples
    â”œâ”€â”€ DRAW
    â”œâ”€â”€ DRAW_L
    â””â”€â”€ ...
â”œâ”€â”€ run.py
â”œâ”€â”€ classifier.py
â”œâ”€â”€ utils.py
â””â”€â”€ ...
</code></pre>
</details>
<p>You can slightly change the <code class="notranslate">test_size</code> and / or<br>
the <code class="notranslate">batch</code> variable value in the <a href="config.txt">config.txt</a> âš™ file.</p>
<h3>Evaluation ğŸ†</h3>
<p>After the fine-tuned model is saved ğŸ’¾, you can explicitly call for evaluation of the model to get a table of TOP-N classes for<br>
the randomly composed subset (10% in size by default) of the training page folder.</p>
<p>There is an option of setting <code class="notranslate">test_size</code> to 0.8 and use all the sorted by category pages provided<br>
in <code class="notranslate">[TRAIN]</code>'s folder for evaluation, but do <strong>NOT</strong> launch it on the whole training data you have actually used up<br>
for the evaluated model training.</p>
<p>To do this in the unchanged configuration âš™, automatically create a<br>
confusion matrix plot ğŸ“Š and additionally get raw class probabilities table run:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --eval
</code></pre>
<p><strong>OR</strong> when you don't remember the specific <code class="notranslate">[SETUP]</code> and <code class="notranslate">[TRAIN]</code> variables' values for the trained model, you can use:</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --eval -model_path 'model_checkpoints/model_&lt;categ_limit&gt;_&lt;base&gt;_&lt;lr&gt;_&lt;epoch&gt;e.pt'
</code></pre>
<p>To prove that initial models without finetuning show awful results you can run <code class="notranslate">--zero_shot</code> flag during the evalution.</p>
<pre class="notranslate"><code class="notranslate">python3 run.py --eval --zero_shot -m '&lt;base_code&gt;'
</code></pre>
<p>Finally, when your model is trained and you are happy with its performance tests, you can uncomment a code line<br>
in the <a href="run.py">run.py</a> ğŸ“ file for <strong>HF ğŸ˜Š hub model push</strong>. This functionality has already been implemented and can be<br>
accessed through the <code class="notranslate">--hf</code> flag using the values set in the <code class="notranslate">[HF]</code> section for the <code class="notranslate">token</code> and <code class="notranslate">repo_name</code> variables.</p>
<p>In this case, you must <strong>rename the trained model folder</strong> in respect to the <code class="notranslate">revision</code> value (dots in the naming are skipped, e.g.<br>
revision <code class="notranslate">v1.9.22</code> turns to <code class="notranslate">model_v1922</code> model folder), and only then run repo push.</p>
<div class="markdown-alert markdown-alert-caution"><p class="markdown-alert-title"><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>Set your own <code class="notranslate">repo_name</code> to the empty one of yours on HF ğŸ˜Š hub, then in the <strong>Settings</strong> of your HF ğŸ˜Š account<br>
find the <strong>Access Tokens</strong> section and generate a new token - copy and paste its value to the <code class="notranslate">token</code> variable. Before committing<br>
those <a href="config.txt">config.txt</a> âš™ file changes via git replace the full <code class="notranslate">token</code> value with its shortened version for security reasons.</p>
</div>
<hr>
<h2>Contacts ğŸ“§</h2>
<p><strong>For support write to:</strong> <a href="mailto:lutsai.k@gmail.com">lutsai.k@gmail.com</a> responsible for this GitHub repository <sup><a href="#user-content-fn-8-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-8-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup> ğŸ”—</p>
<blockquote>
<p>Information about the authors of this project, including their names and ORCIDs, can<br>
be found in the <a href="CITATION.cff">CITATION.cff</a> ğŸ“ file.</p>
</blockquote>
<h2>Acknowledgements ğŸ™</h2>
<ul>
<li><strong>Developed by</strong> UFAL <sup><a href="#user-content-fn-7-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-7-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">15</a></sup> ğŸ‘¥</li>
<li><strong>Funded by</strong> ATRIUM <sup><a href="#user-content-fn-4-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-4-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">16</a></sup>  ğŸ’°</li>
<li><strong>Shared by</strong> ATRIUM <sup><a href="#user-content-fn-4-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-4-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">16</a></sup> &amp; UFAL <sup><a href="#user-content-fn-7-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-7-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">15</a></sup> ğŸ”—</li>
<li><strong>Model type:</strong> fine-tuned CLIP-ViT with a 224x224 <sup><a href="#user-content-fn-2-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-2-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> <sup><a href="#user-content-fn-13-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-13-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> <sup><a href="#user-content-fn-14-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-14-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> ğŸ”— or 336x336 <sup><a href="#user-content-fn-15-be99ecbb8dcfe5011d8d33c2d3ca60da" id="user-content-fnref-15-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> ğŸ”— resolution size</li>
</ul>
<p><strong>Â©ï¸ 2022 UFAL &amp; ATRIUM</strong></p>
<hr>
<h2>Appendix ğŸ¤“</h2>
<details>
<summary>README emoji codes ğŸ‘€</summary>
<ul>
<li>ğŸ–¥ - your computer</li>
<li>ğŸª§ - label/category/class</li>
<li>ğŸ“„ - page/file</li>
<li>ğŸ“ - folder/directory</li>
<li>ğŸ“Š - generated diagrams or plots</li>
<li>ğŸŒ³ - tree of file structure</li>
<li>âŒ› - time-consuming process</li>
<li>âœï¸ - manual action</li>
<li>ğŸ† - performance measurement</li>
<li>ğŸ˜Š - Hugging Face (HF)</li>
<li>ğŸ“§ - contacts</li>
<li>ğŸ‘€ - click to see</li>
<li>âš™ï¸ - configuration/settings</li>
<li>ğŸ“ - link to the internal file</li>
<li>ğŸ”— - link to the external website</li>
</ul>
</details>
<details>
<summary>Content specific emoji codes ğŸ‘€</summary>
<ul>
<li>ğŸ“ - table content</li>
<li>ğŸ“ˆ - drawings/paintings/diagrams</li>
<li>ğŸŒ„ - photos</li>
<li>âœï¸ - handwritten content</li>
<li>ğŸ“„ - text content</li>
<li>ğŸ“° - mixed types of text content, maybe with graphics</li>
</ul>
</details>
<details>
<summary>Decorative emojis ğŸ‘€</summary>
<ul>
<li>ğŸ“‡ğŸ“œğŸ”§â–¶ğŸª„ğŸª›ï¸ğŸ“¦ğŸ”ğŸ“šğŸ™ğŸ‘¥ğŸ“¬ğŸ¤“ - decorative purpose only</li>
</ul>
</details>
<div class="markdown-alert markdown-alert-tip"><p class="markdown-alert-title"><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p>
    <p>Alternative version of this README file is available in <a href="README.md">README.md</a> ğŸ“ markdown</p>
</div>
<section data-footnotes="" class="footnotes"><h2 id="footnote-label" class="sr-only">Footnotes</h2>
<ol>
<li id="user-content-fn-1-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://huggingface.co/ufal/clip-historical-page">https://huggingface.co/ufal/clip-historical-page</a> <a href="#user-content-fnref-1-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-1-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 1-2" class="data-footnote-backref">â†©<sup>2</sup></a> <a href="#user-content-fnref-1-3-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 1-3" class="data-footnote-backref">â†©<sup>3</sup></a> <a href="#user-content-fnref-1-4-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 1-4" class="data-footnote-backref">â†©<sup>4</sup></a> <a href="#user-content-fnref-1-5-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 1-5" class="data-footnote-backref">â†©<sup>5</sup></a> <a href="#user-content-fnref-1-6-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 1-6" class="data-footnote-backref">â†©<sup>6</sup></a></p>
</li>
<li id="user-content-fn-2-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://huggingface.co/openai/clip-vit-base-patch16">https://huggingface.co/openai/clip-vit-base-patch16</a> <a href="#user-content-fnref-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-2-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 2-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-13-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://huggingface.co/openai/clip-vit-base-patch32">https://huggingface.co/openai/clip-vit-base-patch32</a> <a href="#user-content-fnref-13-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-13-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 3-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-14-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://huggingface.co/openai/clip-vit-large-patch14">https://huggingface.co/openai/clip-vit-large-patch14</a> <a href="#user-content-fnref-14-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-14-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 4-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-15-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://huggingface.co/openai/clip-vit-large-patch14-336">https://huggingface.co/openai/clip-vit-large-patch14-336</a> <a href="#user-content-fnref-15-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-15-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 5-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-16-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="http://hdl.handle.net/20.500.12800/1-5959">http://hdl.handle.net/20.500.12800/1-5959</a> <a href="#user-content-fnref-16-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-10-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://developer.nvidia.com/cuda-python">https://developer.nvidia.com/cuda-python</a> <a href="#user-content-fnref-10-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-10-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 7-2" class="data-footnote-backref">â†©<sup>2</sup></a> <a href="#user-content-fnref-10-3-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 7-3" class="data-footnote-backref">â†©<sup>3</sup></a></p>
</li>
<li id="user-content-fn-3-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://docs.python.org/3/library/venv.html">https://docs.python.org/3/library/venv.html</a> <a href="#user-content-fnref-3-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 8" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-5-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://imagemagick.org/script/download.php#windows">https://imagemagick.org/script/download.php#windows</a> <a href="#user-content-fnref-5-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 9" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-5-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 9-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-6-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://www.ghostscript.com/releases/gsdnld.html">https://www.ghostscript.com/releases/gsdnld.html</a> <a href="#user-content-fnref-6-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 10" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-9-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://www.openoffice.org/download/">https://www.openoffice.org/download/</a> <a href="#user-content-fnref-9-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 11" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-11-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments">https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments</a> <a href="#user-content-fnref-11-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 12" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-12-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://pytorch.org/vision/0.20/transforms.html">https://pytorch.org/vision/0.20/transforms.html</a> <a href="#user-content-fnref-12-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 13" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-12-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 13-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-8-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://github.com/ufal/atrium-page-classification">https://github.com/ufal/atrium-page-classification</a> <a href="#user-content-fnref-8-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 14" class="data-footnote-backref">â†©</a></p>
</li>
<li id="user-content-fn-7-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://ufal.mff.cuni.cz/home-page">https://ufal.mff.cuni.cz/home-page</a> <a href="#user-content-fnref-7-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 15" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-7-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 15-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-4-be99ecbb8dcfe5011d8d33c2d3ca60da">
<p><a href="https://atrium-research.eu/">https://atrium-research.eu/</a> <a href="#user-content-fnref-4-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 16" class="data-footnote-backref">â†©</a> <a href="#user-content-fnref-4-2-be99ecbb8dcfe5011d8d33c2d3ca60da" data-footnote-backref="" aria-label="Back to reference 16-2" class="data-footnote-backref">â†©<sup>2</sup></a></p>
</li>
</ol>
</section></article>
	</body>
</html>
